# Autoscaling settings
autoscaling_scale_down_enabled: "true"
autoscaling_buffer_cpu: "1m"
autoscaling_buffer_memory: "10Mi"
autoscaling_buffer_pods: "1"
cluster_autoscaler_cpu: "100m"
cluster_autoscaler_memory: "300Mi"
autoscaling_utilization_threshold: "1.0"
autoscaling_max_empty_bulk_delete: "10"
autoscaling_scale_down_unneeded_time: "10m"

# How long to wait for pod eviction when scaling down.
{{if eq .Cluster.Environment "production"}}
cluster_autoscaler_max_pod_eviction_time: "1h"
{{else}}
cluster_autoscaler_max_pod_eviction_time: "3h"
{{end}}

# ALB config created by kube-aws-ingress-controller
kube_aws_ingress_controller_ssl_policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
kube_aws_ingress_controller_idle_timeout: "1m"
{{if eq .Cluster.Environment "e2e"}}
kube_aws_ingress_controller_deregistration_delay_timeout: "1m"
{{else}}
kube_aws_ingress_controller_deregistration_delay_timeout: "5m"
{{end}}
# allow using NLBs for ingress
# This opens port 9999 (skipper-ingress) on all worker nodes.
kube_aws_ingress_controller_nlb_enabled: "false"
kube_aws_ingress_controller_nlb_cross_zone: "true"

# skipper ingress settings
skipper_ingress_target_average_utilization_cpu: "60"
skipper_ingress_target_average_utilization_memory: "80"
skipper_ingress_max_replicas: "180"
skipper_ingress_min_replicas: "3"
skipper_ingress_cpu: "1000m"
skipper_ingress_memory: "1Gi"
enable_dedicate_nodepool_skipper: "false"
skipper_suppress_route_update_logs: "true"
{{if eq .Cluster.Environment "e2e"}}
skipper_topology_spread_enabled: "true"
{{else}}
skipper_topology_spread_enabled: "false"
{{end}}

# skipper default filters
skipper_default_filters: 'enableAccessLog(4,5) -> lifo(2000,20000,"3s")'

# skipper backend timeout defaults
skipper_expect_continue_timeout_backend: "30s"
skipper_keepalive_backend: "30s"
skipper_max_idle_connection_backend: "0"
skipper_response_header_timeout_backend: "1m"
skipper_timeout_backend: "1m"
skipper_tls_timeout_backend: "1m"
skipper_close_idle_conns_period: "20s"

# skipper server timeout defaults
skipper_idle_timeout_server: "62s"
skipper_read_timeout_server: "5m"
skipper_write_timeout_server: "60s"

# skipper startup settings
{{if eq .Cluster.Environment "production"}}
skipper_readiness_init_delay_seconds: 60
skipper_liveness_init_delay_seconds: 30
{{else}}
skipper_readiness_init_delay_seconds: 1
skipper_liveness_init_delay_seconds: 30
{{end}}

# skipper redis settings
enable_dedicate_nodepool_skipper_redis: "false"
skipper_redis_replicas: 2
skipper_redis_cpu: "100m"
skipper_redis_memory: "512Mi"
skipper_redis_dial_timeout: "25ms"
skipper_redis_pool_timeout: "250ms"
skipper_redis_read_timeout: "25ms"
skipper_redis_write_timeout: "25ms"

# skipper api GW features
enable_apimonitoring: "true"                       # TODO(sszuecs): cleanup candidate to reduce amount of branches in deployment

# skipper east-west feature
# enable_skipper_eastwest is the legacy feature gate for the automatic
# ingress.cluster.local addresses created by skipper.
# enable_skipper_eastwest_dns only enables DNS and assumes users define the
# ingress.cluster.local names explicitly on ingress/routegroup/stacksets
enable_skipper_eastwest_dns: "true"
enable_skipper_eastwest: "false"
enable_skipper_eastwest_range: "true"

# enable temporay logging of ingress.cluster.local names
# used to find services for which it's being used.
skipper_eastwest_dns_log_enabled: "false"

# skipper tcp lifo
# See: https://opensource.zalando.com/skipper/operation/operation/#tcp-lifo
skipper_enable_tcp_queue: "true"                    # TODO(sszuecs): cleanup candidate to reduce amount of branches in deployment
skipper_expected_bytes_per_request: "51200"
skipper_max_tcp_listener_concurrency: "-1"
skipper_max_tcp_listener_queue: "-1"

# opentracing
skipper_ingress_opentracing_excluded_proxy_tags: "skipper.route"
skipper_ingress_opentracing_backend_name_tag: "true"
# lightstep
skipper_ingress_tracing_buffer: "32768"
skipper_ingress_lightstep_grpc_max_msg_size: 16384000
skipper_ingress_lightstep_min_period: "500ms"
skipper_ingress_lightstep_max_period: "2500ms"
skipper_ingress_lightstep_max_log_key_len: 20
skipper_ingress_lightstep_max_log_value_len: 128
skipper_ingress_lightstep_max_logs_per_span: 20
skipper_ingress_lightstep_propagators: "lightstep"
# set to "log-events" to enable
skipper_ingress_lightstep_log_events: ""
lightstep_token: ""
tracing_collector_host: "tracing.stups.zalan.do"

# disabled|provisioned|enabled routegroup validation ( skipper webhook )
# can be one of disabled|provisioned|enabled
routegroups_validation: "enabled"

# enable hostnames with port in host definition
skipper_routegroup_enable_hostport: "false"

# tokeninfo
skipper_ingress_tokeninfo_cpu: "1000m"
skipper_ingress_tokeninfo_memory: "512Mi"
{{if eq .Cluster.Environment "production"}}
tokeninfo_url: "http://127.0.0.1:9021/oauth2/tokeninfo"
# production|bridge|disabled
skipper_local_tokeninfo: "production"
{{else}}
tokeninfo_url: "" # can be set when local tokeninfo is disabled
# production|bridge|disabled
skipper_local_tokeninfo: "bridge"
{{end}}

# oauth2 UI login - grant flow
{{if eq .Cluster.Environment "e2e"}}
skipper_oauth2_ui_login: "false"
skipper_ingress_encryption_key: ""
{{else}}
skipper_oauth2_ui_login: "true"
{{end}}

# Image Policy Webhook
{{if eq .Cluster.Environment "production"}}
image_policy: "trusted"
{{else}}
image_policy: "dev"
{{end}}

# cadvisor settings
cadvisor_cpu: "150m"
cadvisor_memory: "150Mi"

# node exporter settings
node_exporter_cpu: "20m"
node_exporter_memory: "75Mi"

# kube-proxy settings
kube_proxy_cpu: "50m"
kube_proxy_memory: "200Mi"

# flannel settings
flannel_cpu: "25m"
flannel_memory: "100Mi"

# journald reader settings
journald_reader_enabled: "true"
journald_reader_cpu: "1m"
journald_reader_memory: "30Mi"

# Logging settings
logging_s3_bucket: "zalando-logging-{{.InfrastructureAccount | getAWSAccountID}}-{{.Region}}"
scalyr_team_token: ""
log_destination_infra: "scalyr/stups"
log_destination_both: "scalyr/*"
log_destination_local: "scalyr/default"

vpa_cpu: "200m"
vpa_mem: "500Mi"

prometheus_cpu: "1000m"
prometheus_mem: "4Gi"
prometheus_mem_min: "2Gi"
prometheus_cpu_min: "0"
prometheus_tsdb_retention_size: "disabled"
prometheus_csi_ebs: "false"

# Upstream defaults are too aggressive:
# https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
prometheus_remote_write: "disabled"
# Maximum time a sample will wait in buffer.
prometheus_remote_batch_send_deadline: "30s"
# Initial retry delay. Gets doubled for every retry.
prometheus_remote_min_backoff: "3s"
# Maximum retry delay.
prometheus_remote_max_backoff: "10s"

# regex defining labels to be dropped when scraping from kube-state-metrics
# Simple workaround for: https://github.com/kubernetes/kube-state-metrics/issues/1115
# TODO: remove when kube-state-metrics is fixed
prometheus_kube_state_metrics_drop_labels: ""

metrics_service_cpu: "100m"
metrics_service_mem: "200Mi"
metrics_service_mem_max: "4Gi"

kube_aws_iam_controller_cpu: "5m"
kube_aws_iam_controller_mem: "50Mi"
kube_aws_iam_controller_mem_max: "1Gi"

kube_state_metrics_cpu: "100m"
kube_state_metrics_mem: "200Mi"
kube_state_metrics_mem_max: "4Gi"
kube_state_metrics_mem_min: "120Mi"

kubernetes_lifecycle_metrics_mem_max: "4Gi"
kubernetes_lifecycle_metrics_mem_min: "120Mi"

kube_node_ready_controller_cpu: "50m"
kube_node_ready_controller_memory: "200Mi"

# Kubernetes Downscaler (for non-production clusters)
{{if eq .Cluster.Environment "test"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else if eq .Cluster.Environment "e2e"}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "false"
{{end}}

# HPA settings (defaults from https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
horizontal_pod_autoscaler_sync_period: "30s"
horizontal_pod_autoscaler_tolerance: "0.1"
horizontal_pod_downscale_stabilization: "5m0s"

# Cluster update settings
{{if eq .Cluster.Environment "production"}}
drain_grace_period: "6h"
drain_min_pod_lifetime: "72h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "6h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "true"
{{else}}
drain_grace_period: "2h"
drain_min_pod_lifetime: "8h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "1h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "false" # don't wait for a replacement instance for on-demand pools in test clusters
{{end}}
# add NoSchedule taints to nodes being replaced
decommission_node_no_schedule_taint: "true"

# Teapot admission controller
teapot_admission_controller_default_cpu_request: "25m"
teapot_admission_controller_default_memory_request: "100Mi"
teapot_admission_controller_process_resources: "true"
teapot_admission_controller_application_min_creation_time: "2019-06-03T12:00:00Z"
teapot_admission_controller_ndots: "2"
teapot_admission_controller_inject_environment_variables: "true"
teapot_admission_controller_deployment_default_max_surge: "5%"
teapot_admission_controller_deployment_default_max_unavailable: "1"
teapot_admission_controller_inject_aws_waiter: "true"
teapot_admission_controller_parent_resource_hash: "true"

## Defaults are set per-cluster
teapot_admission_controller_check_daemonset_resources: "true"
teapot_admission_controller_daemonset_reserved_cpu: "8"
teapot_admission_controller_daemonset_reserved_memory: "64Gi"

{{if eq .Cluster.Environment "production"}}
teapot_admission_controller_validate_application_label: "true"
teapot_admission_controller_validate_base_images: "true"
teapot_admission_controller_validate_pod_template_resources: "true"
teapot_admission_controller_preemption_enabled: "true"
teapot_admission_controller_postgresql_delete_protection_enabled: "true"
teapot_admission_controller_namespace_delete_protection_enabled: "true"
{{else if eq .Cluster.Environment "e2e"}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_base_images: "false"
teapot_admission_controller_validate_pod_template_resources: "false"
teapot_admission_controller_preemption_enabled: "true"
teapot_admission_controller_postgresql_delete_protection_enabled: "false"
teapot_admission_controller_namespace_delete_protection_enabled: "false"
{{else}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_base_images: "false"
teapot_admission_controller_validate_pod_template_resources: "true"
teapot_admission_controller_preemption_enabled: "false"
teapot_admission_controller_postgresql_delete_protection_enabled: "false"
teapot_admission_controller_namespace_delete_protection_enabled: "false"
{{end}}

{{if eq .Cluster.Environment "e2e"}}
teapot_admission_controller_ignore_namespaces: "^kube-system|((downward-api|kubectl|projected|statefulset|pod-network|scope-selectors|resourcequota|limitrange)-.*)$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "false"
{{else}}
teapot_admission_controller_ignore_namespaces: "^kube-system$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "true"
{{end}}

# Enable kube-node-ready-controller and node-not-ready taint
teapot_admission_controller_node_not_ready_taint: "true"

# Some third-party controllers use API groups that look like they belong to Kubernetes resources. Explicitly allow them anyway.
teapot_admission_controller_crd_role_provisioning_allowed_api_groups: "flink.k8s.io"

teapot_admission_controller_topology_spread: optin

# Supported providers: 'zalando' or 'spotio'
teapot_admission_controller_node_lifecycle_provider: "zalando"

# Enable and configure runtime-policy annotation
{{if eq .Cluster.Environment "production"}}
teapot_admission_controller_runtime_policy_enabled: "false"
teapot_admission_controller_runtime_policy_default: "require-on-demand"
{{else}}
teapot_admission_controller_runtime_policy_enabled: "true"
teapot_admission_controller_runtime_policy_default: "allow-spot"
{{end}}
# Enforce a certain policy (<empty>|allow-spot|require-on-demand) for a cluster,
# leave empty for falling back to the default.
teapot_admission_controller_runtime_policy_enforced: ""
# Enable hard spot assignment. Only relevant when node_lifecycle_provider=zalando
teapot_admission_controller_runtime_policy_spot_hard_assignment: "false"

# Enable and configure prevent scale down annotation
{{if eq .Cluster.Environment "production"}}
teapot_admission_controller_prevent_scale_down_enabled: "false"
teapot_admission_controller_prevent_scale_down_allowed: "true"
{{else}}
teapot_admission_controller_prevent_scale_down_enabled: "true"
teapot_admission_controller_prevent_scale_down_allowed: "false"
{{end}}

# etcd cluster
{{if eq .Cluster.Environment "production"}}
etcd_instance_count: "5"
{{else}}
etcd_instance_count: "3"
{{end}}

etcd_scalyr_key: ""
dynamodb_service_link_enabled: "false"

cluster_dns: "coredns"
coredns_log_svc_names: "true"
# max concurrency for upstream (AWS VPC) DNS server
#
# AWS VPC DNS server has a limit of 1024 qps before packets are dropped.
# This setting is tuned to allow a buffer compared to the normal DNS QPS in our
# clusters and prevent CoreDNS from running out of memory in case of spikes.
coredns_max_upstream_concurrency: 2000 # 0 means there is no concurrency limits


# Kubernetes on Ubuntu AMI to use
# note this configuration uses the [amiID][0] function. It returns the
# AMI id given the image name and the Image AWS account owner.
#
# [0]: https://github.com/zalando-incubator/cluster-lifecycle-manager/blob/8a9bd1cb2d094038a9e23e646421f8146b48886a/provisioner/template.go#L116
kuberuntu_image_v1_19: {{ amiID "zalando-ubuntu-kubernetes-production-v1.19.8-master-148" "861068367966"}}

# Feature toggle for auditing events
audit_pod_events: "true"
{{if eq .Cluster.Environment "production"}}
audittrail_url: "https://audittrail.cloud.zalando.com"
{{else}}
audittrail_url: ""
{{end}}
audittrail_root_account_role: ""

# CIDR configuration for nodes and pods
# Changing this will change the number of nodes and pods we can schedule in the
# cluster: https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
{{if eq .Cluster.Environment "production"}}
node_cidr_mask_size: "25"
{{else}}
node_cidr_mask_size: "24"
{{end}}
# How many nodes to keep reserved (e.g. to allow for increasing the node_cidr_mask_size).
# Note that this only affects CA settings, someone can still scale up the ASGs manually.
reserved_nodes: "5"

# maximum number of PIDs allowed to be allocated per pod
pod_max_pids: "4096"

# the cpu management policy which should be used by the kubelet
cpu_manager_policy: "none"

# sysctl names allowed to be used in security policies, comma-separated
allowed_unsafe_sysctls: "net.ipv4.tcp_keepalive_time,net.ipv4.tcp_keepalive_intvl,net.ipv4.tcp_keepalive_probes,net.ipv4.tcp_syn_retries,net.ipv4.tcp_retries2"

# enable CSIMigration feature flag
enable_csi_migration: "false"

# pull images in parallel
serialize_image_pulls: "false"

# when set to true, routes external traffic to the apiserver through a skipper sidecar
apiserver_proxy: "true"

# defines the rollout status of the NLB for the API server. The options are:
#
#   disabled:    no NLB will be provisioned
#   provisioned: NLB will be provisioned but not hooked up to the ASG
#   hooked:      NLB will be provisioned and hooked up to the ASG but DNS still points to the ELB
#   active:      NLB will be fully functional and DNS points to the NLB
#   promoted:    NLB will be fully functional and ELB will be unhooked
#   exclusive:   NLB will be fully functional and ELB will be removed
#
apiserver_nlb: "exclusive"

# defines temporary rollout of NLB with TCP healthcheck. This assumes
# apiserver_nlb=exclusive. The options are:
#
#   hooked:    New NLB is hooked up to ASG but DNS still points to old NLB
#   active:    New NLB will be fully functional and DNS points to the new NLB
#   promoted:  New NLB will be fully functional and old NLB will be unhooked
#   exclusive: New NLB will be fully functional and old NLB will be removed
#
apiserver_nlb_tcp: "hooked"

# when set to true, service account tokens can be used from outside the cluster
# requires apiserver_proxy to be set to "true"
allow_external_service_accounts: "false"
# issue service account tokens with expiration time.
rotate_service_account_tokens: "false"

# enable auditlogging for read access such that we can identified clients using
# the default service account to read from the API server.
{{ if eq .Cluster.Environment "test" }}
auditlog_read_access: "true"
{{ else }}
auditlog_read_access: "false"
{{ end }}

# allow ssh access for internal VPC IPs only
ssh_vpc_only: "false"

# configure custom dns zone
custom_dns_zone: "" # zone name e.g. example.org
custom_dns_zone_nameservers: "" # space seperated list of nameserver IP addresses

# prefix prepended to ownership TXT records for external-dns
external_dns_ownership_prefix: ""
# domains that should be ignored by ExternalDNS
external_dns_excluded_domains: cluster.local

# select which cache to use for Cluster DNS
{{ if eq .Cluster.Environment "production" }}
dns_cache: "dnsmasq"
{{ else }}
dns_cache: "unbound"
{{ end }}

# DNS container resources
dns_dnsmasq_cpu: "100m"
dns_dnsmasq_mem: "50Mi"
dns_dnsmasq_sidecar_cpu: "10m"
dns_dnsmasq_sidecar_mem: "45Mi"
dns_unbound_cpu: "100m"
dns_unbound_mem: "50Mi"
dns_unbound_telemetry_cpu: "10m"
dns_unbound_telemetry_mem: "45Mi"
dns_coredns_cpu: "50m"
dns_coredns_mem: "100Mi"

# special roles for test/pet clusters
{{if eq .Cluster.Environment "e2e"}}
collaborator_administrator_access: "true"
{{else}}
collaborator_administrator_access: "false"
{{end}}

# enable legacy serviceaccounts for smooth RBAC migration
enable_operator_sa: "false"
enable_default_sa: "false"

# virtual memory configuration
vm_dirty_background_bytes: "67108864"
vm_dirty_bytes: "134217728"

# enable kube-proxy to use endpoint slice
enable_endpointsliceproxying: "true"

# Enable FeatureGate HPAScaleToZero
enable_hpa_scale_to_zero: "true"

# Enable FeatureGate EphemeralContainers (Alpha)
# https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/
enable_ephemeral_containers: "false"

# enable encryption of secrets in etcd
# this flag can be switched between true and false
# to ensure all secrets are encrypted/decrypted all secrets need to be rewritten after masters have been rolled
enable_encryption: "true"

# Enable the feature gate GenericEphemeralVolume
# https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes
enable_generic_ephemeral_volume: "false"

# Enable the feature gate SetHostnameAsFQDN
# https://v1-19.docs.kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-sethostnameasfqdn-field
enable_hostname_as_fqdn: "false"

# default ttl for kube janitor for resources build from PRs in namespaces matching .*-pr-.*
kube_janitor_default_pr_ttl: "1w"  # 1 week
# opt-in deletion of unused PVCs
kube_janitor_default_unused_pvc_ttl: "forever"

# deletes all resources in the cluster that rely on a vpc
# necessary to change the VPC subnet of a cluster
delete_vpc_resources: "false"
# replacement strategy used for default on-demand worker pool
on_demand_worker_replacement_strategy: none

# SpotAllocationStrategy for pools
spot_allocation_strategy: "capacity-optimized"

# Stackset controller
stackset_controller_sync_interval: "10s"
stackset_controller_mem_min: "120Mi"
stackset_controller_mem_max: "4Gi"

# EBS settings for the root volume
ebs_master_root_volume_size: "50"
ebs_root_volume_size: "50"
ebs_root_volume_delete_on_termination: "true"

# Priority class used for critical system pods
system_priority_class: "cluster-critical-nonpreempting"

# spot.io Ocean configuration.
#
# Default configuration per ocean, can be configured on individual node pools.
spotio_ocean_spot_percentage: "100"
spotio_spot_percentage: "100" # per node pool setting
spotio_ocean_fallback_to_ondemand: "true"
spotio_ocean_utilize_reserved_instances: "false"

# configuration for spot.io controller
spotio_ocean_controller_cpu: "50m"
spotio_ocean_controller_memory: "512Mi"

# Log Kubernetes events to Scalyr
kubernetes_event_logger_enabled: "true"

# enable/disable routegroup support for stackset
stackset_routegroup_support_enabled: "true"
# The ttl before an ingress source is deleted when replaced with another
# one.
# E.g. switching from RouteGroup to Ingress or vice versa.
stackset_ingress_source_switch_ttl: "5m"

# Enable/Disable profiling for Kubernetes components
enable_control_plane_profiling: "false"

# Enable use of Privileged PSP (running privileged pods) in non system namespaces.
# TODO: remove once all clusters have been migrated away from privileged pods
privileged_psp_enabled: "false"

# TODO: remove after CLM is updated
clm_new_userdata_path: "true"

{{if eq .Cluster.Environment "production"}}
experimental_proxy_use_serviceaccount: "false"
{{else}}
experimental_proxy_use_serviceaccount: "true"
{{end}}
