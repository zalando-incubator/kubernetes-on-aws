# Autoscaling settings
autoscaling_scale_down_enabled: "true"
autoscaling_buffer_cpu: "1m"
autoscaling_buffer_memory: "10Mi"
autoscaling_buffer_pods: "1"
cluster_autoscaler_cpu: "100m"
cluster_autoscaler_memory: "300Mi"
autoscaling_utilization_threshold: "1.0"
autoscaling_max_empty_bulk_delete: "10"
autoscaling_scale_down_unneeded_time: "10m"

# the cluster autoscaler release to use, options are 1_12 and 1_18.
cluster_autoscaler_release: "1_18"

# When true, enables the legacy behaviour for 1.18 where pools that are backed off
# don't get reset to their pre-scale-out values. This basically means the ASGs
# keep trying to get a node while CA already moves on to the next ASG.
cluster_autoscaler_keep_backed_off_pools_scaled_up: "true"

# How long to wait for pod eviction when scaling down.
{{if eq .Environment "production"}}
cluster_autoscaler_max_pod_eviction_time: "2m"
cluster_autoscaler_disable_node_instances_cache: "false"
{{else}}
cluster_autoscaler_max_pod_eviction_time: "3h"
cluster_autoscaler_disable_node_instances_cache: "true"
{{end}}

# defines which expander the autoscaler should use
cluster_autoscaler_expander: highest-priority

# ALB config created by kube-aws-ingress-controller
kube_aws_ingress_controller_ssl_policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
kube_aws_ingress_controller_idle_timeout: "1m"
{{if eq .Environment "e2e"}}
kube_aws_ingress_controller_deregistration_delay_timeout: "1m"
{{else}}
kube_aws_ingress_controller_deregistration_delay_timeout: "5m"
{{end}}
# allow using NLBs for ingress
# This opens port 9999 (skipper-ingress) on all worker nodes.
kube_aws_ingress_controller_nlb_enabled: "false"
kube_aws_ingress_controller_nlb_cross_zone: "true"

# skipper ingress settings
skipper_ingress_target_average_utilization_cpu: "60"
skipper_ingress_target_average_utilization_memory: "80"
skipper_ingress_max_replicas: "180"
skipper_ingress_min_replicas: "3"
skipper_ingress_cpu: "1000m"
skipper_ingress_memory: "1Gi"
skipper_ingress_tracing_buffer: "8192"
enable_dedicate_nodepool_skipper: "false"
{{if eq .Environment "e2e"}}
skipper_topology_spread_enabled: "true"
{{else}}
skipper_topology_spread_enabled: "false"
{{end}}

# skipper default filters
skipper_default_filters: 'enableAccessLog(4,5) -> lifo(2000,20000,"3s")'

# skipper backend timeout defaults
skipper_expect_continue_timeout_backend: "30s"
skipper_keepalive_backend: "30s"
skipper_max_idle_connection_backend: "0"
skipper_response_header_timeout_backend: "1m"
skipper_timeout_backend: "1m"
skipper_tls_timeout_backend: "1m"
skipper_close_idle_conns_period: "20s"

# skipper server timeout defaults
skipper_idle_timeout_server: "62s"
skipper_read_timeout_server: "5m"
skipper_write_timeout_server: "60s"

# skipper redis settings
skipper_redis_replicas: 2
skipper_redis_cpu: "100m"
skipper_redis_memory: "100Mi"

# skipper api GW features
enable_apimonitoring: "true"                       # TODO(sszuecs): cleanup candidate to reduce amount of branches in deployment

# skipper east-west feature
# enable_skipper_eastwest is the legacy feature gate for the automatic
# ingress.cluster.local addresses created by skipper.
# enable_skipper_eastwest_dns only enables DNS and assumes users define the
# ingress.cluster.local names explicitly on ingress/routegroup/stacksets
{{if eq .Environment "production"}}
enable_skipper_eastwest: "false"
enable_skipper_eastwest_dns: "false"
{{else}}
enable_skipper_eastwest: "false"
enable_skipper_eastwest_dns: "true"
{{end}}
# enable temporay logging of ingress.cluster.local names
# used to find services for which it's being used.
skipper_eastwest_dns_log_enabled: "false"

# skipper tcp lifo
# See: https://opensource.zalando.com/skipper/operation/operation/#tcp-lifo
skipper_enable_tcp_queue: "true"                    # TODO(sszuecs): cleanup candidate to reduce amount of branches in deployment
skipper_expected_bytes_per_request: "51200"
skipper_max_tcp_listener_concurrency: "-1"
skipper_max_tcp_listener_queue: "-1"

# opentracing
skipper_ingress_opentracing_excluded_proxy_tags: "skipper.route"
skipper_ingress_opentracing_backend_name_tag: "true"
# lightstep
skipper_ingress_lightstep_grpc_max_msg_size: 16384000
skipper_ingress_lightstep_min_period: "500ms"
skipper_ingress_lightstep_max_period: "2500ms"
# set to "log-events" to enable
skipper_ingress_lightstep_log_events: ""
lightstep_token: ""
tracing_collector_host: "tracing.stups.zalan.do"

# disabled|provisioned|enabled routegroup validation ( skipper webhook )
# can be one of disabled|provisioned|enabled
routegroups_validation: "enabled"

# enable hostnames with port in host definition
skipper_routegroup_enable_hostport: "false"

# migrate from multi container to single container multi process skipper-ingress
skipper_single_container_enabled: "true"

# tokeninfo
skipper_ingress_tokeninfo_cpu: "1000m"
skipper_ingress_tokeninfo_memory: "512Mi"
{{if eq .Environment "production"}}
tokeninfo_url: "http://127.0.0.1:9021/oauth2/tokeninfo"
# production|bridge|disabled
skipper_local_tokeninfo: "production"
{{else}}
tokeninfo_url: "" # can be set when local tokeninfo is disabled
# production|bridge|disabled
skipper_local_tokeninfo: "bridge"
{{end}}

# Image Policy Webhook
{{if eq .Environment "production"}}
image_policy: "trusted"
{{else}}
image_policy: "dev"
{{end}}

# cadvisor settings
cadvisor_cpu: "150m"
cadvisor_memory: "150Mi"

# node exporter settings
node_exporter_cpu: "20m"
node_exporter_memory: "75Mi"

# kube-proxy settings
kube_proxy_cpu: "50m"
kube_proxy_memory: "200Mi"

# flannel settings
flannel_cpu: "25m"
flannel_memory: "100Mi"

# journald reader settings
{{if eq .Environment "production"}}
journald_reader_enabled: "false"
{{else}}
journald_reader_enabled: "true"
{{end}}
journald_reader_cpu: "1m"
journald_reader_memory: "30Mi"

# Logging settings
logging_s3_bucket: "zalando-logging-{{.InfrastructureAccount | getAWSAccountID}}-{{.Region}}"
scalyr_team_token: ""

vpa_cpu: "200m"
vpa_mem: "500Mi"

prometheus_cpu: "1000m"
prometheus_mem: "4Gi"
prometheus_mem_min: "2Gi"
prometheus_cpu_min: "0"
prometheus_remote_write: "disabled"
prometheus_tsdb_retention_size: "disabled"
prometheus_csi_ebs: "false"

# regex defining labels to be dropped when scraping from kube-state-metrics
# Simple workaround for: https://github.com/kubernetes/kube-state-metrics/issues/1115
# TODO: remove when kube-state-metrics is fixed
prometheus_kube_state_metrics_drop_labels: ""

metrics_service_cpu: "100m"
metrics_service_mem: "200Mi"
metrics_service_mem_max: "4Gi"

kube_aws_iam_controller_cpu: "5m"
kube_aws_iam_controller_mem: "50Mi"
kube_aws_iam_controller_mem_max: "1Gi"

kube_state_metrics_cpu: "100m"
kube_state_metrics_mem: "200Mi"
kube_state_metrics_mem_max: "4Gi"
kube_state_metrics_mem_min: "120Mi"

kubernetes_lifecycle_metrics_mem_max: "4Gi"
kubernetes_lifecycle_metrics_mem_min: "120Mi"

# Kubernetes Downscaler (for non-production clusters)
{{if eq .Environment "test"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else if eq .Environment "e2e"}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "false"
{{end}}

# HPA settings (defaults from https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
horizontal_pod_autoscaler_sync_period: "30s"
horizontal_pod_autoscaler_tolerance: "0.1"
horizontal_pod_downscale_stabilization: "5m0s"

# Cluster update settings
{{if eq .Environment "production"}}
drain_grace_period: "6h"
drain_min_pod_lifetime: "72h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "6h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "true"
{{else}}
drain_grace_period: "2h"
drain_min_pod_lifetime: "8h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "1h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "false" # don't wait for a replacement instance for on-demand pools in test clusters
{{end}}

# Teapot admission controller
teapot_admission_controller_default_cpu_request: "25m"
teapot_admission_controller_default_memory_request: "100Mi"
teapot_admission_controller_process_resources: "true"
teapot_admission_controller_application_min_creation_time: "2019-06-03T12:00:00Z"
teapot_admission_controller_ndots: "2"
teapot_admission_controller_inject_environment_variables: "true"
teapot_admission_controller_deployment_default_max_surge: "5%"
teapot_admission_controller_deployment_default_max_unavailable: "1"
teapot_admission_controller_inject_aws_waiter: "true"
teapot_admission_controller_parent_resource_hash: "true"
teapot_admission_controller_postgresql_delete_protection_enabled: "false"

## Defaults are set per-cluster
teapot_admission_controller_check_daemonset_resources: "true"
teapot_admission_controller_daemonset_reserved_cpu: "8"
teapot_admission_controller_daemonset_reserved_memory: "64Gi"

{{if eq .Environment "production"}}
teapot_admission_controller_validate_application_label: "true"
teapot_admission_controller_validate_pod_template_resources: "true"
teapot_admission_controller_preemption_enabled: "true"
{{else if eq .Environment "e2e"}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_pod_template_resources: "false"
teapot_admission_controller_preemption_enabled: "true"
{{else}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_pod_template_resources: "true"
teapot_admission_controller_preemption_enabled: "false"
{{end}}

{{if eq .Environment "e2e"}}
teapot_admission_controller_ignore_namespaces: "^kube-system|((downward-api|kubectl|projected|statefulset|pod-network|scope-selectors|resourcequota|limitrange)-.*)$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "false"
{{else}}
teapot_admission_controller_ignore_namespaces: "^kube-system$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "true"
{{end}}

teapot_admission_controller_topology_spread: optin

# etcd cluster
{{if eq .Environment "production"}}
etcd_instance_count: "5"
{{else}}
etcd_instance_count: "3"
{{end}}

etcd_scalyr_key: ""
dynamodb_service_link_enabled: "false"

cluster_dns: "coredns"
coredns_log_svc_names: "true"
coredns_max_upstream_concurrency: 0 #0 means there is not concurrency limits

kuberuntu_image_v1_18: {{ amiID "zalando-ubuntu-kubernetes-production-v1.18.10-master-129" "861068367966" }}
kuberuntu_image_v1_19: {{ amiID "zalando-ubuntu-kubernetes-production-v1.19.3-master-129" "861068367966" }}

# Feature toggle for auditing events
audit_pod_events: "true"
{{if eq .Environment "production"}}
audittrail_url: "https://audittrail.cloud.zalando.com"
{{else}}
audittrail_url: ""
{{end}}
audittrail_root_account_role: ""

# CIDR configuration for nodes and pods
# Changing this will change the number of nodes and pods we can schedule in the
# cluster: https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
{{if eq .Environment "production"}}
node_cidr_mask_size: "25"
{{else}}
node_cidr_mask_size: "24"
{{end}}
# How many nodes to keep reserved (e.g. to allow for increasing the node_cidr_mask_size).
# Note that this only affects CA settings, someone can still scale up the ASGs manually.
reserved_nodes: "5"

# maximum number of PIDs allowed to be allocated per pod
pod_max_pids: "4096"

# the cpu management policy which should be used by the kubelet
cpu_manager_policy: "none"

# enable CSIMigration feature flag
enable_csi_migration: "false"

# pull images in parallel
serialize_image_pulls: "false"

# when set to true, routes external traffic to the apiserver through a skipper sidecar
apiserver_proxy: "true"

# defines the rollout status of the NLB for the API server. The options are:
#
#   disabled:    no NLB will be provisioned
#   provisioned: NLB will be provisioned but not hooked up to the ASG
#   hooked:      NLB will be provisioned and hooked up to the ASG but DNS still points to the ELB
#   active:      NLB will be fully functional and DNS points to the NLB
#   promoted:    NLB will be fully functional and ELB will be unhooked
#   exclusive:   NLB will be fully functional and ELB will be removed
#
apiserver_nlb: "exclusive"

# when set to true, service account tokens can be used from outside the cluster
# requires apiserver_proxy to be set to "true"
allow_external_service_accounts: "false"
# issue service account tokens with expiration time.
rotate_service_account_tokens: "false"

# enable auditlogging for read access such that we can identified clients using
# the default service account to read from the API server.
{{ if eq .Cluster.Environment "test" }}
auditlog_read_access: "true"
{{ else }}
auditlog_read_access: "false"
{{ end }}

# enable automatic injection of OIDC-based AWS API access
teapot_admission_controller_service_account_iam: "true"
# only configure the userdata part of the OIDC-based AWS API access feature, useful for rolling back
# has no effect when teapot_admission_controller_service_account_iam is true
teapot_admission_controller_service_account_iam_userdata: "true"
# use kube-aws-iam-controller for kube-system components
kube_aws_iam_controller_kube_system_enable: "true"

# allow ssh access for internal VPC IPs only
ssh_vpc_only: "false"

# configure custom dns zone
custom_dns_zone: "" # zone name e.g. example.org
custom_dns_zone_nameservers: "" # space seperated list of nameserver IP addresses

# prefix prepended to ownership TXT records for external-dns
external_dns_ownership_prefix: ""
# domains that should be ignored by ExternalDNS
external_dns_excluded_domains: cluster.local

# DNS container resources
dns_dnsmasq_cpu: "100m"
dns_dnsmasq_mem: "50Mi"
dns_dnsmasq_sidecar_cpu: "10m"
dns_dnsmasq_sidecar_mem: "45Mi"
dns_coredns_cpu: "50m"
dns_coredns_mem: "100Mi"

# special roles for test/pet clusters
{{if eq .Cluster.Environment "e2e"}}
collaborator_administrator_access: "true"
{{else}}
collaborator_administrator_access: "false"
{{end}}

# enable legacy serviceaccounts for smooth RBAC migration
enable_operator_sa: "false"
enable_default_sa: "false"
enable_cdp_sa: "false"

# virtual memory configuration
vm_dirty_background_bytes: "67108864"
vm_dirty_bytes: "134217728"

# enable kube-proxy to use endpoint slice
enable_endpointsliceproxying: "true"

# Enable FeatureGate HPAScaleToZero
enable_hpa_scale_to_zero: "true"

# Enable FeatureGate EphemeralContainers (Alpha)
# https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/
enable_ephemeral_containers: "false"

# enable encryption of secrets in etcd
# this flag can be switched between true and false
# to ensure all secrets are encrypted/decrypted all secrets need to be rewritten after masters have been rolled
enable_encryption: "true"

# default ttl for kube janitor for resources build from PRs in namespaces matching .*-pr-.*
kube_janitor_default_pr_ttl: "1w"  # 1 week
# opt-in deletion of unused PVCs
kube_janitor_default_unused_pvc_ttl: "forever"

# deletes all resources in the cluster that rely on a vpc
# necessary to change the VPC subnet of a cluster
delete_vpc_resources: "false"
# replacement strategy used for default on-demand worker pool
on_demand_worker_replacement_strategy: none

# SpotAllocationStrategy for pools
spot_allocation_strategy: "capacity-optimized"

# Stackset controller
stackset_controller_sync_interval: "10s"
stackset_controller_mem_min: "120Mi"
stackset_controller_mem_max: "4Gi"

# EBS settings for the root volume
ebs_root_volume_size: "50"
ebs_root_volume_delete_on_termination: "true"

# Priority class used for critical system pods
system_priority_class: "cluster-critical-nonpreempting"

# spot.io Ocean configuration.
#
# Default configuration per ocean, can be configured on individual node pools.
spotio_ocean_spot_percentage: "100"
spotio_ocean_fallback_to_ondemand: "true"
spotio_ocean_utilize_reserved_instances: "false"

# configuration for spot.io controller
spotio_ocean_controller_cpu: "50m"
spotio_ocean_controller_memory: "512Mi"

# Log Kubernetes events to Scalyr
kubernetes_event_logger_enabled: "true"

# enable/disable routegroup support for stackset
stackset_routegroup_support_enabled: "true"

# Enable/Disable profiling for Kubernetes components
enable_control_plane_profiling: "false"
