# Autoscaling settings
autoscaling_scale_down_enabled: "true"
autoscaling_buffer_cpu: "2"
autoscaling_buffer_memory: "10Gi"
autoscaling_buffer_pods: "0"
cluster_autoscaler_cpu: "100m"
cluster_autoscaler_memory: "300Mi"
autoscaling_utilization_threshold: "1.0"
autoscaling_max_empty_bulk_delete: "25"
autoscaling_scale_down_unneeded_time: "10m"

# defines which expander the autoscaler should use
cluster_autoscaler_expander: highest-priority

# ALB config created by kube-aws-ingress-controller
kube_aws_ingress_controller_ssl_policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
kube_aws_ingress_controller_idle_timeout: "1m"
# allow using NLBs for ingress
# This opens port 9999 (skipper-ingress) on all worker nodes.
kube_aws_ingress_controller_nlb_enabled: "false"
kube_aws_ingress_controller_nlb_cross_zone: "true"

# skipper ingress settings
skipper_ingress_target_average_utilization_cpu: "60"
skipper_ingress_target_average_utilization_memory: "80"
skipper_ingress_max_replicas: "30"
skipper_ingress_min_replicas: "3"
skipper_ingress_cpu: "1000m"
skipper_ingress_memory: "1Gi"
skipper_ingress_tracing_buffer: "8192"
enable_dedicate_nodepool_skipper: "false"

# skipper default filters
skipper_default_filters: 'enableAccessLog(4,5) -> lifo(2000,20000,"3s")'

# skipper backend timeout defaults
skipper_expect_continue_timeout_backend: "30s"
skipper_keepalive_backend: "30s"
skipper_max_idle_connection_backend: "0"
skipper_response_header_timeout_backend: "1m"
skipper_timeout_backend: "1m"
skipper_tls_timeout_backend: "1m"
skipper_close_idle_conns_period: "20s"

# skipper server timeout defaults
skipper_idle_timeout_server: "62s"
skipper_read_timeout_server: "5m"
skipper_write_timeout_server: "60s"

# skipper api GW features
enable_apimonitoring: "true"
skipper_clusterratelimit: "true"
{{if eq .Environment "production"}}
enable_skipper_eastwest: "false"
{{else}}
enable_skipper_eastwest: "true"
{{end}}

# skipper tcp lifo
# See: https://opensource.zalando.com/skipper/operation/operation/#tcp-lifo
skipper_enable_tcp_queue: "true"
skipper_expected_bytes_per_request: "51200"
skipper_max_tcp_listener_concurrency: "-1"
skipper_max_tcp_listener_queue: "-1"

# opentracing
skipper_ingress_opentracing_excluded_proxy_tags: "skipper.route"
# lightstep
skipper_ingress_lightstep_grpc_max_msg_size: 16384000
skipper_ingress_lightstep_min_period: "500ms"
skipper_ingress_lightstep_max_period: "2500ms"
# set to "log-events" to enable
skipper_ingress_lightstep_log_events: ""
lightstep_token: ""

# tokeninfo
skipper_ingress_tokeninfo_cpu: "1000m"
skipper_ingress_tokeninfo_memory: "512Mi"
{{if eq .Environment "production"}}
tokeninfo_url: "https://info.services.auth.zalando.com/oauth2/tokeninfo"
opendid_provider_cfg_url: "https://planb-provider.greendale-questionmark.zalan.do/.well-known/openid-configuration"
openid_issuer: "https://identity.zalando.com"
{{else}}
tokeninfo_url: "https://sandbox-tokeninfo-bridge.stups.zalan.do/oauth2/tokeninfo"
opendid_provider_cfg_url: "https://sandbox.identity.zalando.com/.well-known/openid-configuration"
openid_issuer: "https://sandbox.identity.zalando.com"
{{end}}

# Image Policy Webhook
{{if eq .Environment "production"}}
image_policy: "trusted"
{{else}}
image_policy: "dev"
{{end}}

# cadvisor settings
cadvisor_cpu: "150m"
cadvisor_memory: "150Mi"

# node exporter settings
node_exporter_cpu: "20m"
node_exporter_memory: "75Mi"

# Logging settings
logging_s3_bucket: "zalando-logging-{{.InfrastructureAccount | getAWSAccountID}}-{{.Region}}"
scalyr_team_token: ""

prometheus_cpu: "1000m"
prometheus_mem: "4Gi"
prometheus_mem_min: "2Gi"
prometheus_cpu_min: "0"
prometheus_remote_write: "disabled"
prometheus_tsdb_retention_size: "disabled"
prometheus_csi_ebs: "false"

# regex defining labels to be dropped when scraping from kube-state-metrics
# Simple workaround for: https://github.com/kubernetes/kube-state-metrics/issues/1115
# TODO: remove when kube-state-metrics is fixed
prometheus_kube_state_metrics_drop_labels: ""

metrics_service_cpu: "100m"
metrics_service_mem: "200Mi"
metrics_service_mem_max: "1Gi"

kube_aws_iam_controller_cpu: "5m"
kube_aws_iam_controller_mem: "50Mi"
kube_aws_iam_controller_mem_max: "1Gi"

kube_state_metrics_cpu: "100m"
kube_state_metrics_mem: "200Mi"
kube_state_metrics_mem_max: "1Gi"

# Kubernetes Downscaler (for non-production clusters)
{{if eq .Environment "test"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else if eq .Environment "e2e"}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "true"
{{else}}
downscaler_default_uptime: "always"
downscaler_default_downtime: "never"
downscaler_enabled: "false"
{{end}}

# HPA settings (defaults from https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
horizontal_pod_autoscaler_downscale_delay: "5m0s"
horizontal_pod_autoscaler_sync_period: "30s"
horizontal_pod_autoscaler_tolerance: "0.1"
horizontal_pod_autoscaler_upscale_delay: "3m0s"
horizontal_pod_downscale_stabilization: "5m0s"

# Cluster update settings
{{if eq .Environment "production"}}
drain_grace_period: "6h"
drain_min_pod_lifetime: "72h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "6h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "true"
{{else}}
drain_grace_period: "2h"
drain_min_pod_lifetime: "8h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "1h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "false" # don't wait for a replacement instance for on-demand pools in test clusters
{{end}}

# Teapot admission controller
teapot_admission_controller_default_cpu_request: "25m"
teapot_admission_controller_default_memory_request: "100Mi"
teapot_admission_controller_process_resources: "true"
teapot_admission_controller_application_min_creation_time: "2019-06-03T12:00:00Z"
teapot_admission_controller_ndots: "2"
teapot_admission_controller_inject_environment_variables: "true"
teapot_admission_controller_deployment_default_max_surge: "5%"
teapot_admission_controller_deployment_default_max_unavailable: "1"
teapot_admission_controller_inject_aws_waiter: "true"
teapot_admission_controller_parent_resource_hash: "true"

## Defaults are set per-cluster
teapot_admission_controller_check_daemonset_resources: "true"
teapot_admission_controller_daemonset_reserved_cpu: "8"
teapot_admission_controller_daemonset_reserved_memory: "64Gi"

{{if eq .Environment "production"}}
teapot_admission_controller_validate_application_label: "true"
teapot_admission_controller_validate_pod_template_resources: "true"
{{else if eq .Environment "e2e"}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_pod_template_resources: "false"
{{else}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_validate_pod_template_resources: "true"
{{end}}

{{if eq .Environment "e2e"}}
teapot_admission_controller_ignore_namespaces: "^kube-system|((downward-api|kubectl|projected|statefulset|pod-network|scope-selectors|resourcequota)-.*)$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "false"
{{else}}
teapot_admission_controller_ignore_namespaces: "^kube-system$"
teapot_admission_controller_crd_ensure_no_resources_on_delete: "true"
{{end}}


# etcd cluster
{{if eq .Environment "production"}}
etcd_instance_count: "5"
{{else}}
etcd_instance_count: "3"
{{end}}

etcd_scalyr_key: ""
dynamodb_service_link_enabled: "false"

cluster_dns: "coredns"
coredns_log_svc_names: "true"
coredns_max_upstream_concurrency: 0 #0 means there is not concurrency limits

kuberuntu_image_v1_17: {{ amiID "zalando-ubuntu-kubernetes-production-v1.17.8-master-114" "861068367966" }}

# Feature toggle to allow gradual decommissioning of ingress-template-controller
enable_ingress_template_controller: "false"

# Feature toggle for auditing events
audit_pod_events: "true"
{{if eq .Environment "production"}}
audittrail_url: "https://audittrail.cloud.zalando.com"
{{else}}
audittrail_url: ""
{{end}}
audittrail_root_account_role: ""

# CIDR configuration for nodes and pods
# Changing this will change the number of nodes and pods we can schedule in the
# cluster: https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
{{if eq .Environment "production"}}
node_cidr_mask_size: "25"
{{else}}
node_cidr_mask_size: "24"
{{end}}
# How many nodes to keep reserved (e.g. to allow for increasing the node_cidr_mask_size).
# Note that this only affects CA settings, someone can still scale up the ASGs manually.
reserved_nodes: "5"

# maximum number of PIDs allowed to be allocated per pod
pod_max_pids: "4096"

# the cpu management policy which should be used by the kubelet
cpu_manager_policy: "none"

# enable CSIMigration feature flag
enable_csi_migration: "false"

# pull images in parallel
serialize_image_pulls: "false"

# when set to true, routes external traffic to the apiserver through a skipper sidecar
apiserver_proxy: "true"

# when set to true, service account tokens can be used from outside the cluster
# requires apiserver_proxy to be set to "true"
allow_external_service_accounts: "false"
# issue service account tokens with expiration time.
rotate_service_account_tokens: "false"

# enable auditlogging for read access such that we can identified clients using
# the default service account to read from the API server.
{{ if eq .Cluster.Environment "test" }}
auditlog_read_access: "true"
{{ else }}
auditlog_read_access: "false"
{{ end }}

# enable automatic injection of OIDC-based AWS API access
teapot_admission_controller_service_account_iam: "true"
# only configure the userdata part of the OIDC-based AWS API access feature, useful for rolling back
# has no effect when teapot_admission_controller_service_account_iam is true
teapot_admission_controller_service_account_iam_userdata: "true"
# use kube-aws-iam-controller for kube-system components
kube_aws_iam_controller_kube_system_enable: "true"

# allow ssh access for internal VPC IPs only
ssh_vpc_only: "false"

# configure custom dns zone
custom_dns_zone: "" # zone name e.g. example.org
custom_dns_zone_nameservers: "" # space seperated list of nameserver IP addresses

# prefix prepended to ownership TXT records for external-dns
external_dns_ownership_prefix: ""

# DNS container resources
dns_dnsmasq_cpu: "100m"
dns_dnsmasq_mem: "50Mi"
dns_dnsmasq_sidecar_cpu: "10m"
dns_dnsmasq_sidecar_mem: "45Mi"
dns_coredns_cpu: "50m"
dns_coredns_mem: "100Mi"

# special roles for test/pet clusters
{{if eq .Cluster.Environment "e2e"}}
collaborator_administrator_access: "true"
{{else}}
collaborator_administrator_access: "false"
{{end}}

# enable legacy serviceaccounts for smooth RBAC migration
enable_operator_sa: "false"
enable_default_sa: "false"
enable_cdp_sa: "false"

# virtual memory configuration
vm_dirty_background_bytes: "67108864"
vm_dirty_bytes: "134217728"

# Enable FeatureGate EndpointSlice
enable_endpointslice: "false"

# Enable FeatureGate HPAScaleToZero
enable_hpa_scale_to_zero: "true"

# enable encryption of secrets in etcd
# this flag can be switched between true and false
# to ensure all secrets are encrypted/decrypted all secrets need to be rewritten after masters have been rolled
enable_encryption: "true"

# default ttl for kube janitor for resources build from PRs in namespaces matching .*-pr-.*
kube_janitor_default_pr_ttl: "1w"  # 1 week
# opt-in deletion of unused PVCs
kube_janitor_default_unused_pvc_ttl: "forever"

# deletes all resources in the cluster that rely on a vpc
# necessary to change the VPC subnet of a cluster
delete_vpc_resources: "false"
# replacement strategy used for default on-demand worker pool
on_demand_worker_replacement_strategy: none

# SpotAllocationStrategy for pools
spot_allocation_strategy: "capacity-optimized"

# Stackset controller
stackset_controller_sync_interval: "10s"

# EBS settings for the root volume
ebs_root_volume_size: "50"

# Migration off priority classes
{{if eq .Environment "production"}}
system_pods_critical: "true"
{{else}}
system_pods_critical: "false"
{{end}}

# spot.io Ocean configuration.
#
# Default configuration per ocean, can be configured on individual node pools.
spotio_ocean_spot_percentage: "100"
spotio_ocean_fallback_to_ondemand: "true"
spotio_ocean_utilize_reserved_instances: "false"

# configuration for spot.io controller
spotio_ocean_controller_cpu: "50m"
spotio_ocean_controller_memory: "512Mi"

# Log Kubernetes events to Scalyr
kubernetes_event_logger_enabled: "true"

# enable/disable stackset validation (preserveUnknownFields)
stackset_validation_enabled: "true"
