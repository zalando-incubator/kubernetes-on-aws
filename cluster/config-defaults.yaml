# Autoscaling settings
autoscaling_scale_down_enabled: "true"
autoscaling_buffer_pools: "default-worker"
autoscaling_buffer_cpu_scale: "1"
autoscaling_buffer_memory_scale: "0.85"
autoscaling_buffer_cpu_reserved: "1250m"
autoscaling_buffer_memory_reserved: "3Gi"
{{if eq .Environment "production"}}
autoscaling_buffer_pods: "1"
{{else}}
autoscaling_buffer_pods: "0"
{{end}}

# skipper resource settings
skipper_limits_mem: "250Mi"
skipper_requests_cpu: "150m"
skipper_requests_mem: "50Mi"

# skipper ingress settings
skipper_ingress_target_average_utilization_cpu: "100"
skipper_ingress_target_average_utilization_memory: "80"
skipper_ingress_max_replicas: "30"
skipper_ingress_min_replicas: "3"
skipper_ingress_cpu: "500m"
skipper_ingress_memory: "500Mi"
skipper_ingress_tracing_buffer: "8192"

# skipper default filters
skipper_default_filters: 'enableAccessLog(4,5) -> lifo(2000,20000,"10s")'

# skipper backend timeout defaults
skipper_expect_continue_timeout_backend: "30s"
skipper_keepalive_backend: "30s"
skipper_max_idle_connection_backend: "0"
skipper_response_header_timeout_backend: "1m"
skipper_timeout_backend: "1m"
skipper_tls_timeout_backend: "1m"
skipper_close_idle_conns_period: "20s"
skipper_idle_timeout_server: "60s"

# skipper api GW features
enable_apimonitoring: "true"
{{if eq .Environment "production"}}
skipper_clusterratelimit: "false"
enable_skipper_eastwest: "false"
{{else}}
skipper_clusterratelimit: "true"
enable_skipper_eastwest: "true"
{{end}}

# lightstep
{{if eq .Environment "production"}}
lightstep_token: "aws:kms:AQICAHgrx06TPoR1aNmcPHJjFu5mmoICT5KJkx2fsTJpmXmbNAH+8Ml18b8ZkUO/0KAwtIZTAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMSf79AuT/RI5rvWWjAgEQgDuN7obV7JD4iBMnOJ4Th93DfM5j572dXjf+gWmHx4JKMTTJPX2w6hgfQXX3LjI49l0p479a6IXIlZJOSg=="
{{else}}
lightstep_token: "aws:kms:AQICAHgrx06TPoR1aNmcPHJjFu5mmoICT5KJkx2fsTJpmXmbNAHvvYXdV1r7NviF5S+Jyx5zAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMQBqSQk/2TuQOsHGOAgEQgDsrNbCwF4AxoQXZuxXUOPnuQhFCY02EhWcB4xqmjFy8DelZtiCldRtxRdLyDL4uXiEyV8vOFyhxgqso/A=="
{{end}}

# tokeninfo
{{if eq .Environment "production"}}
tokeninfo_url: "https://info.services.auth.zalando.com/oauth2/tokeninfo"
{{else}}
tokeninfo_url: "https://sandbox-tokeninfo-bridge.stups.zalan.do/oauth2/tokeninfo"
{{end}}

# Image Policy Webhook
{{if eq .Environment "production"}}
image_policy: "trusted"
{{else}}
image_policy: "dev"
{{end}}
compliance_checker_enabled: "true"

# Egress configuration
nat_cidr_blocks: "172.31.64.0/28,172.31.64.16/28,172.31.64.32/28"

# cadvisor settings
cadvisor_cpu: "150m"
cadvisor_memory: "150Mi"

# Monitoring settings
{{if eq .Environment "e2e"}}
logging_agent_enabled: "false"
zmon_agent_replicas: "0"
zmon_aws_agent_replicas: "0"
zmon_redis_replicas: "0"
zmon_scheduler_replicas: "0"
zmon_worker_replicas: "0"
zmon_worker_mem: "0Gi"
zmon_worker_cpu: "0"
zmon_worker_count: "0"
{{else if eq .Environment "production"}}
logging_agent_enabled: "true"
zmon_agent_replicas: "1"
zmon_aws_agent_replicas: "1"
zmon_redis_replicas: "1"
zmon_scheduler_replicas: "1"
zmon_worker_replicas: "6"
zmon_worker_mem: "4Gi"
zmon_worker_cpu: "750m"
zmon_worker_count: "16"
{{else}}
logging_agent_enabled: "true"
zmon_agent_replicas: "1"
zmon_aws_agent_replicas: "1"
zmon_redis_replicas: "1"
zmon_scheduler_replicas: "1"
zmon_worker_replicas: "4"
zmon_worker_mem: "4Gi"
zmon_worker_cpu: "750m"
zmon_worker_count: "16"
{{end}}
logging_watcher_mem: "200Mi"
logging_scalyr_mem: "200Mi"
logging_fluentd_mem: "300Mi"
logging_watcher_cpu: "50m"
logging_scalyr_cpu: "100m"
logging_fluentd_cpu: "100m"
logging_s3_bucket: "zalando-logging-{{.InfrastructureAccount | getAWSAccountID}}-{{.Region}}"

zmon_redis_mem: "1Gi"
zmon_agent_mem: "500Mi"
zmon_agent_cpu: "200m"
zmon_aws_agent_mem: "200Mi"
zmon_aws_agent_cpu: "100m"

prometheus_cpu: "1000m"
prometheus_mem: "4Gi"
prometheus_mem_min: "512Mi"
prometheus_mem_max: "10Gi"

metrics_service_cpu: "100m"
metrics_service_mem: "200Mi"
metrics_service_mem_max: "1Gi"

kube_state_metrics_cpu: "100m"
kube_state_metrics_mem: "200Mi"
kube_state_metrics_mem_max: "1Gi"

# Kubernetes Downscaler (for non-production clusters)
{{if eq .Environment "test"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
{{else if eq .Environment "playground"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
{{else}}
downscaler_default_uptime: "always"
{{end}}

# HPA settings (defaults from https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
horizontal_pod_autoscaler_downscale_delay: "5m0s"
horizontal_pod_autoscaler_sync_period: "30s"
horizontal_pod_autoscaler_tolerance: "0.1"
horizontal_pod_autoscaler_upscale_delay: "3m0s"

# Cluster update settings
{{if eq .Environment "production"}}
drain_grace_period: "6h"
drain_min_pod_lifetime: "72h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "6h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "true"
{{else}}
drain_grace_period: "2h"
drain_min_pod_lifetime: "8h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "1h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "false" # don't wait for a replacement instance for on-demand pools in test clusters
{{end}}

# Temporary feature toggles for the cluster lifecycle controller
experimental_cluster_lifecycle_controller: "false"

# Teapot admission controller
teapot_admission_controller_default_cpu_request: "25m"
teapot_admission_controller_default_memory_request: "100Mi"
teapot_admission_controller_process_resources: "true"
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_application_min_creation_time: "2999-12-31T23:59:59Z"
teapot_admission_controller_ndots: "2"

{{if eq .Environment "e2e"}}
teapot_admission_controller_ignore_namespaces: "^kube-system|(e2e-tests-(downward-api|kubectl|projected|statefulset|pod-network)-.*)$"
{{else}}
teapot_admission_controller_ignore_namespaces: "^kube-system$"
{{end}}

# etcd cluster
{{if ne .Environment "production"}}
etcd_instance_count: "3"
{{end}}

dynamodb_service_link_enabled: "false"

cluster_dns: "coredns"
coredns_log_svc_names: "true"

coreos_image: "ami-06142be4afc3d5b1b" # Container Linux 2079.4.0 (HVM, eu-central-1)

# Feature toggle to allow gradual decommissioning of ingress-template-controller
enable_ingress_template_controller: "false"

# Temporary feature toggle for the new daemonset scheduler
{{if eq .Environment "e2e"}}
experimental_schedule_daemonset_pods: "true"
{{else}}
experimental_schedule_daemonset_pods: "false"
{{end}}

# Feature toggle for auditing events
audit_pod_events: "true"

# CIDR configuration for nodes and pods
# Changing this will change the number of nodes and pods we can schedule in the
# cluster
# The two flags should be changed together to have the right balance between
# available pod IP addresses per node and maximum pods per node. Roughly the
# double amount of IP addresses should be available.
# https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
# flag passed to kubelet
# TODO: pass this flag to worker kublets (will do it with 1.13 to prevent extra
# rolling of nodes)
node_max_pods: "110" # Default: 110
# flag passed to controller-manager
node_cidr_mask_size: "24" # Default: 24
