# Autoscaling settings
autoscaling_scale_down_enabled: "true"
autoscaling_buffer_pools: "default-worker"
autoscaling_buffer_cpu_scale: "1"
autoscaling_buffer_memory_scale: "0.85"
autoscaling_buffer_cpu_reserved: "1250m"
autoscaling_buffer_memory_reserved: "3Gi"
{{if eq .Environment "production"}}
autoscaling_buffer_pods: "1"
{{else}}
autoscaling_buffer_pods: "0"
{{end}}
cluster_autoscaler_cpu: "100m"
cluster_autoscaler_memory: "300Mi"

# defines which expander the autoscaler should use
cluster_autoscaler_expander: highest-priority

# ALB config created by kube-aws-ingress-controller
kube_aws_ingress_controller_ssl_policy: "ELBSecurityPolicy-TLS-1-2-2017-01"
kube_aws_ingress_controller_idle_timeout: "1m"
# allow using NLBs for ingress
# This opens port 9999 (skipper-ingress) on all worker nodes.
kube_aws_ingress_controller_nlb_enabled: "false"
kube_aws_ingress_controller_nlb_cross_zone: "true"

# skipper ingress settings
skipper_ingress_target_average_utilization_cpu: "70"
skipper_ingress_target_average_utilization_memory: "80"
skipper_ingress_max_replicas: "30"
skipper_ingress_min_replicas: "3"
skipper_ingress_cpu: "1000m"
skipper_ingress_memory: "1Gi"
skipper_ingress_tracing_buffer: "8192"

# skipper default filters
skipper_default_filters: 'enableAccessLog(4,5) -> lifo(2000,20000,"3s")'

# skipper backend timeout defaults
skipper_expect_continue_timeout_backend: "30s"
skipper_keepalive_backend: "30s"
skipper_max_idle_connection_backend: "0"
skipper_response_header_timeout_backend: "1m"
skipper_timeout_backend: "1m"
skipper_tls_timeout_backend: "1m"
skipper_close_idle_conns_period: "20s"

# skipper server timeout defaults
skipper_idle_timeout_server: "62s"
skipper_read_timeout_server: "5m"
skipper_write_timeout_server: "60s"

# skipper api GW features
enable_apimonitoring: "true"
skipper_clusterratelimit: "true"
{{if eq .Environment "production"}}
enable_skipper_eastwest: "false"
{{else}}
enable_skipper_eastwest: "true"
{{end}}

# skipper tcp lifo
# See: https://opensource.zalando.com/skipper/operation/operation/#tcp-lifo
{{if eq .Environment "production"}}
skipper_enable_tcp_queue: "false"
{{else}}
skipper_enable_tcp_queue: "true"
{{end}}
skipper_expected_bytes_per_request: "51200"
skipper_max_tcp_listener_concurrency: "-1"
skipper_max_tcp_listener_queue: "-1"

# opentracing
skipper_ingress_opentracing_excluded_proxy_tags: "skipper.route"
# lightstep
skipper_ingress_lightstep_grpc_max_msg_size: 16384000
skipper_ingress_lightstep_min_period: "500ms"
skipper_ingress_lightstep_max_period: "2500ms"
# set to "log-events" to enable
skipper_ingress_lightstep_log_events: ""
{{if eq .Environment "production"}}
lightstep_token: "aws:kms:AQICAHgrx06TPoR1aNmcPHJjFu5mmoICT5KJkx2fsTJpmXmbNAH+8Ml18b8ZkUO/0KAwtIZTAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMSf79AuT/RI5rvWWjAgEQgDuN7obV7JD4iBMnOJ4Th93DfM5j572dXjf+gWmHx4JKMTTJPX2w6hgfQXX3LjI49l0p479a6IXIlZJOSg=="
{{else}}
lightstep_token: "aws:kms:AQICAHgrx06TPoR1aNmcPHJjFu5mmoICT5KJkx2fsTJpmXmbNAHvvYXdV1r7NviF5S+Jyx5zAAAAfjB8BgkqhkiG9w0BBwagbzBtAgEAMGgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMQBqSQk/2TuQOsHGOAgEQgDsrNbCwF4AxoQXZuxXUOPnuQhFCY02EhWcB4xqmjFy8DelZtiCldRtxRdLyDL4uXiEyV8vOFyhxgqso/A=="
{{end}}

# tokeninfo
skipper_ingress_tokeninfo_cpu: "1000m"
skipper_ingress_tokeninfo_memory: "512Mi"
{{if eq .Environment "production"}}
tokeninfo_url: "https://info.services.auth.zalando.com/oauth2/tokeninfo"
opendid_provider_cfg_url: "https://planb-provider.greendale-questionmark.zalan.do/.well-known/openid-configuration"
openid_issuer: "https://identity.zalando.com"
{{else}}
tokeninfo_url: "https://sandbox-tokeninfo-bridge.stups.zalan.do/oauth2/tokeninfo"
opendid_provider_cfg_url: "https://sandbox.identity.zalando.com/.well-known/openid-configuration"
openid_issuer: "https://sandbox.identity.zalando.com"
{{end}}

# Image Policy Webhook
{{if eq .Environment "production"}}
image_policy: "trusted"
{{else}}
image_policy: "dev"
{{end}}

# cadvisor settings
cadvisor_cpu: "150m"
cadvisor_memory: "150Mi"

# node exporter settings
node_exporter_cpu: "20m"
node_exporter_memory: "75Mi"

# Monitoring settings
{{if eq .Environment "e2e"}}
logging_agent_enabled: "false"
zmon_agent_replicas: "0"
zmon_aws_agent_replicas: "0"
zmon_redis_replicas: "0"
zmon_scheduler_replicas: "0"
zmon_scheduler_mem: "0Gi"
zmon_worker_replicas: "0"
zmon_worker_mem: "0Gi"
zmon_worker_cpu: "0"
zmon_worker_count: "0"
{{else if eq .Environment "production"}}
logging_agent_enabled: "true"
zmon_agent_replicas: "1"
zmon_aws_agent_replicas: "1"
zmon_redis_replicas: "1"
zmon_scheduler_replicas: "1"
zmon_scheduler_mem: "2.5Gi"
zmon_worker_replicas: "6"
zmon_worker_mem: "4Gi"
zmon_worker_cpu: "750m"
zmon_worker_count: "16"
{{else}}
logging_agent_enabled: "true"
zmon_agent_replicas: "1"
zmon_aws_agent_replicas: "1"
zmon_redis_replicas: "1"
zmon_scheduler_replicas: "1"
zmon_scheduler_mem: "2.5Gi"
zmon_worker_replicas: "4"
zmon_worker_mem: "4Gi"
zmon_worker_cpu: "750m"
zmon_worker_count: "16"
{{end}}
zmon_scalyr_region: "eu"
zmon_worker_version: "v209-py2eol-53-g458f8ba-v251-py2eol"
zmon_agent_version: "0.4-19-ga12da10-zv5"
logging_watcher_mem: "200Mi"
logging_scalyr_mem: "175Mi"
logging_slo_heartbeat_mem: "25Mi"
logging_fluentd_mem: "300Mi"
logging_watcher_cpu: "50m"
logging_scalyr_cpu: "90m"
logging_slo_heartbeat_cpu: "10m"
logging_fluentd_cpu: "100m"
logging_s3_bucket: "zalando-logging-{{.InfrastructureAccount | getAWSAccountID}}-{{.Region}}"
scalyr_team_token: ""
logging_agent_version: "v0.37"
logging_watcher_version: "master-68"
logging_scalyr_enable_profiling: "false"

zmon_redis_mem: "1Gi"
zmon_agent_mem: "500Mi"
zmon_agent_cpu: "200m"
zmon_aws_agent_mem: "200Mi"
zmon_aws_agent_cpu: "100m"

prometheus_cpu: "1000m"
prometheus_mem: "4Gi"
prometheus_mem_min: "1Gi"
prometheus_cpu_min: "0"

metrics_service_cpu: "100m"
metrics_service_mem: "200Mi"
metrics_service_mem_max: "1Gi"

kube_aws_iam_controller_cpu: "5m"
kube_aws_iam_controller_mem: "50Mi"
kube_aws_iam_controller_mem_max: "1Gi"

kube_state_metrics_cpu: "100m"
kube_state_metrics_mem: "200Mi"
kube_state_metrics_mem_max: "1Gi"

# Kubernetes Downscaler (for non-production clusters)
{{if eq .Environment "test"}}
downscaler_default_uptime: "Mon-Fri 07:30-20:30 Europe/Berlin"
{{else}}
downscaler_default_uptime: "always"
{{end}}

# HPA settings (defaults from https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)
horizontal_pod_autoscaler_downscale_delay: "5m0s"
horizontal_pod_autoscaler_sync_period: "30s"
horizontal_pod_autoscaler_tolerance: "0.1"
horizontal_pod_autoscaler_upscale_delay: "3m0s"

# Cluster update settings
{{if eq .Environment "production"}}
drain_grace_period: "6h"
drain_min_pod_lifetime: "72h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "6h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "true"
{{else}}
drain_grace_period: "2h"
drain_min_pod_lifetime: "8h"
drain_min_healthy_sibling_lifetime: "1h"
drain_min_unhealthy_sibling_lifetime: "1h"
drain_force_evict_interval: "5m"
node_update_prepare_replacement_node: "false" # don't wait for a replacement instance for on-demand pools in test clusters
{{end}}

# Teapot admission controller
teapot_admission_controller_default_cpu_request: "25m"
teapot_admission_controller_default_memory_request: "100Mi"
teapot_admission_controller_process_resources: "true"
teapot_admission_controller_application_min_creation_time: "2019-06-03T12:00:00Z"
teapot_admission_controller_ndots: "2"
teapot_admission_controller_inject_environment_variables: "true"
teapot_admission_controller_deployment_default_max_surge: "5%"
teapot_admission_controller_deployment_default_max_unavailable: "1"

{{if eq .Environment "production"}}
teapot_admission_controller_validate_application_label: "true"
teapot_admission_controller_resource_cutoff_timestamp: "2019-07-08T22:00:00Z"
teapot_admission_controller_resource_validate_redeployments: "true"
{{else if eq .Environment "e2e"}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_resource_cutoff_timestamp: ""
teapot_admission_controller_resource_validate_redeployments: "false"
{{else}}
teapot_admission_controller_validate_application_label: "false"
teapot_admission_controller_resource_cutoff_timestamp: "2019-07-08T22:00:00Z"
teapot_admission_controller_resource_validate_redeployments: "true"
{{end}}

{{if eq .Environment "e2e"}}
teapot_admission_controller_ignore_namespaces: "^kube-system|((downward-api|kubectl|projected|statefulset|pod-network)-.*)$"
{{else}}
teapot_admission_controller_ignore_namespaces: "^kube-system$"
{{end}}

# etcd cluster
{{if eq .Environment "production"}}
etcd_instance_count: "5"
{{else}}
etcd_instance_count: "3"
{{end}}

{{if ne .Environment "e2e"}}
etcd_scalyr_key: "aws:kms:AQECAHgNxQ3yghAnRNmN6GyaSh8l0hGHUap3hNqu00tmcgcOxgAAAIwwgYkGCSqGSIb3DQEHBqB8MHoCAQAwdQYJKoZIhvcNAQcBMB4GCWCGSAFlAwQBLjARBAwrD+79PBvwdNnIXGICARCASEVQTLFyRAWL1OWE1WSl19lm5wY6Cj38wWse8esMBlxUvYSDRZqqyKMJieZpDbAxaAnJO6FKEJdpOoyU8GsV8At/43DhtaHkdA=="
{{else}}
etcd_scalyr_key: ""
{{end}}

dynamodb_service_link_enabled: "false"

cluster_dns: "coredns"
coredns_log_svc_names: "true"

kuberuntu_image_v1_14: {{ amiID "zalando-ubuntu-kubernetes-production-v1.14.8-master-77" "861068367966" }}
kuberuntu_image_v1_15: {{ amiID "zalando-ubuntu-kubernetes-production-v1.15.6-master-81" "861068367966" }}

# Feature toggle to allow gradual decommissioning of ingress-template-controller
enable_ingress_template_controller: "false"

# Temporary feature toggle for the new daemonset scheduler
{{if eq .Environment "e2e"}}
experimental_schedule_daemonset_pods: "true"
{{else}}
experimental_schedule_daemonset_pods: "false"
{{end}}

# Feature toggle for auditing events
audit_pod_events: "true"
{{if eq .Environment "production"}}
audittrail_url: "https://audittrail.cloud.zalando.com"
{{else}}
audittrail_url: ""
{{end}}

# Feature toggle for CustomResourceWebhookConversion (alpha in v1.13)
# https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/#webhook-conversion
custom_resource_webhook_conversion: "false"

# Feature toggle for CustomResourcePublishOpenAPI (alpha in v1.14)
# https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#publish-validation-schema-in-openapi-v2
custom_resource_publish_openapi: "false"

# CIDR configuration for nodes and pods
# Changing this will change the number of nodes and pods we can schedule in the
# cluster: https://cloud.google.com/kubernetes-engine/docs/how-to/flexible-pod-cidr
{{if eq .Environment "production"}}
node_cidr_mask_size: "25"
{{else}}
node_cidr_mask_size: "24"
{{end}}
# How many nodes to keep reserved (e.g. to allow for increasing the node_cidr_mask_size).
# Note that this only affects CA settings, someone can still scale up the ASGs manually.
reserved_nodes: "5"

# maximum number of PIDs allowed to be allocated per pod
pod_max_pids: "4096"

# the cpu management policy which should be used by the kubelet
cpu_manager_policy: "none"

# when set to true, routes external traffic to the apiserver through a skipper sidecar
apiserver_proxy: "true"
# when set to true, service account tokens can be used from outside the cluster
# requires apiserver_proxy to be set to "true"
allow_external_service_accounts: "false"

# use kube-aws-iam-controller for kube-system components
kube_aws_iam_controller_kube_system_enable: "true"

# allow ssh access for internal VPC IPs only
ssh_vpc_only: "false"

# configure custom dns zone
custom_dns_zone: "" # zone name e.g. example.org
custom_dns_zone_nameservers: "" # space seperated list of nameserver IP addresses
