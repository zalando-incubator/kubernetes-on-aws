# container linux config
passwd:
  users:
  - name: core
    ssh_authorized_keys:
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAt8djQXfn5U7H85oDzuZRfRONF+jVqn3Mp9t3tnBrJdKyTccfDovq1sekzEdOFdmj74yfS8bzaIO9pDUczy6j5k2MtDCoRnzAO6KZc46jMJ2GjhLArUuHLjmAw2r9LotZ30LEIEvJdmUI7mDlPMczv381PghyM8+DsYv62UrfjDiOsZ4EVkYnztQlO3ntNE16RFNj4fbfErQz67kmw0lB8C6bAf0RuRmvXzB7xRMplmknQnLusoURmySKdZM0GUe0VY6fmqOsgzHVLoEs1m82V8QK1ac/1DSHA91v50MbpCjTVLaRhjR8nmhWBedlhb6j5ClZdAQ8iwyyWC1MFQuvKw== henning@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICZh6WTi6o6ofWqJBmJjzYZdM/La5B/aZQLfpuho6rx7 mikkel.larsen@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIISHc8p8+ycB9H6SI/i3Uu/OI5G3vYNsZTn0DffPvfOA martin.linkhorst@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFVmr+Y3hBvnz2xgSWoaxaPxnExcEXOghUozupU9bKNR nick@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCq3oEP8qhMvGtlR1bgVc9tFOVJ5B5RMKtm6UQ/zXQUpm8DQ04SxdM2U7TfuLien2HSfpHAlYe0eLJZUfIqCXUeZ37v0ozj2RglireEcJm0t9XJ7kTS4kqVxrL6iuN6qQVGHs0vxoo/o9+SP0YkuuJoXwJvVI4yKVbnbfA5hKaAffAYPmfgqOZ7+3AMwmaj/D3tI0xVEA48ptGkj5nnOl0pXlfLRNvbnXOCa/dTKUgkma1F0lXoTipkRspsMEiAnwfJ1dwnzgNzllt//Ao/H+yOVR8fWJ7d+nowszIk6zwUR7c6walxKKf5Oy5bQBU49MZ6xLP1oma9F2+llmV7qpqx rodrigo.reis@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqAikaHsZWMuJvKZphZPZG0fnKMvVCRfBAbIS6e0Y+YqM0PfsWgB5e4f5TrbisQHdKopbfZVwYIaV/NegEuinrYPKC7t2ese/HjxgjHR95zHOcDP19Cbo+xeyH8zbRd9K3iRSyCUSMNRw5NL6zN8JOSl12m8QWQA4hTjFTmt870fIT4RLxu9qGlbQipUm57E/SotsNC41MQ/PsLQzOAviKrkS1rei2vzRHzAcjz1Z7GT5oH+dFVUC66kKa0XWDvq+VtkRVoLvS2chrIPCgESeeZAyOKyiOoyJxFFFiMVK48MWDBBIYTIsHE0qs/RwBi9+8lQGiHK5Rpk2djcloO0c7 sandor.szuecs@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINkh52Py+FvH9CRLDQg0gzvjEIrzwA45yMTXTsl2BVxV alexey.ermakov@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDakbukcXXmMyyxw5h3J8h41FK8b8/qjiX/u4BP+qQpQ3x/1VkppIpzHwLcjVThNspwOKKU6QwcaLKpJ+g3mYWdilsl7eFT5hAiIXq4OLe9XynXp1ZEduqKk7tgvjClo7OzuSdEfKtWkD35hzXWigVfJFZhSN9bLdiEzuIEHf0jEo2mUXADgAzSwbaTcVUreH/IR6SAYBVh4XgKoQjOmWrizJguYNk+DgBW6/R+5JQi8YYPSuevbcEKXXm1ZfOudNv2ebOL1rA4I+VDe2xW+JVwKIvAN3odt0zOH2NFr1kFU1GTVyD2ZYtyTNNae9a+mQ2Ltw5pzKjA4zjFctQrrI+yPEIcf64mccumIoRMjWozPB801VbZqUVyG0XhzHW7RNmTmmLsNa/STtYI9Xaqfrz2n3PgpYJ/bB42l4Ez9Oblh9rZU0aisr6risLh5XXgCa9NezMjTczMHJE0jVINgeIMsCleNF0HrkF26Uo1MJO1pOMZOGZzXSksB79TmTwjRdvQT/uvSAElBrthmRgj82pDsRdNoipp6VMx54m2KbvRyq79PVZP69NGWdAhJBw1SV0447kT1tuoqDTUskS4es1GoSu5sqtW4eF4Q6oju8+01l3ygLKmPPTbFVRcs6bBvHjnB3RYwgai5z0U16rsh7LKUwhYsRiar3lmrhVvoc95sw== muhammad.muaaz.saleem@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKIX5twzJYryUz6bw/0SLJRBg5RNfxUZiXOZ/1E75c2r arjun.naik@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC6QTMJEdUHX/50ZzwelLGouD9mXpeHyTz0FW/vK1NPQHE0OM193bCXy0A+vjf7GaMUq8KOB8bdm96w3ffGM7ZKvyMOh4UHsxojabVZyKCaDafpQhzVSn1CRVy7vyJKoWkE5usuEbD2dKLxY5Svpw78+DHSdjNGTyFXPf/YGqNP2xrjTP96sx8yWg02w49kD3XrBQaFg5YicCHAAAwQ0IObx7dlT+6B9QrAuVY1nhIq1yqmKvLZa6h6MkoQPlp3Y3ksBhs0C+RNpB51EbBw2VMZJ+rbk/pOKDhlOMSDrLJb6TpAo6P7j1lvK0NrKpTrsifNPaJ5GtILa1hjVkMXpxJNDjc6+ApOzIuKhaACaufV/C7yKd1bGYFClkoDqxYck8mgQnBA+MAYWgLr4Bxu/R0Q1JwPiVDd6QGJcMSiBayIR5GT47X8SeV2yrNLBzbc7J3hctYNQxiNc+H2pxLr9P+yFCB6Xql0poFaG8vppAETPxeF1CaY86WsknPA5SqnDipr1O6OLJWcQMWw9iHj9mDMwyNpXJK3T5k/aoE+URZb2GxnTRzzcC3H+yi8ol3NsbyylHTSGwmbfbupzSenh2sSq7aZipDX6ELm0RbuogIbalOAtOUROGiG0HNcG8GbJLky6qMBSyh59VnaCDWPfFde1gsg+SH8Od3y2TOA+1rtbw== jannis.rake-revelant@zalando.de'
systemd:
  units:
  # disable automatic updates
  - name: update-engine.service
    mask: true
  - name: locksmithd.service
    mask: true

  # disable socket activation for sshd
  - name: sshd.service
    enable: true
  - name: sshd.socket
    mask: true

{{if gt (len .Values.instance_info.InstanceStorageDevices) 0 }}
  - name: {{if index .NodePool.ConfigItems "instance_storage_mount_path"}}{{.NodePool.ConfigItems.instance_storage_mount_path | mountUnitName}}{{else}}var-lib-docker{{end}}.mount
    enable: true
    contents: |
      [Unit]
      Before=local-fs.target

      [Mount]
      What={{(index .Values.instance_info.InstanceStorageDevices 0).Path}}
      Where={{if index .NodePool.ConfigItems "instance_storage_mount_path"}}{{.NodePool.ConfigItems.instance_storage_mount_path}}{{else}}/var/lib/docker{{end}}
      Type=ext4

      [Install]
      WantedBy=local-fs.target
{{end}}

  - name: set-hostname.service
    enable: true
    contents: |
      [Unit]
      Wants=network.target
      Before=docker.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/hostnamectl set-hostname $(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-hostname)"

      [Install]
      WantedBy=multi-user.target

  - name: update-system-settings.service
    enable: true
    contents: |
      [Unit]
      Before=docker.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStartPre=/bin/sh -c 'test -f /sys/module/nvme_core/parameters/io_timeout && /usr/bin/echo 4294967295 > /sys/module/nvme_core/parameters/io_timeout || true'
      ExecStart=/bin/sh -c '/usr/bin/echo madvise > /sys/kernel/mm/transparent_hugepage/enabled'

      [Install]
      WantedBy=multi-user.target

  - name: docker.service
    dropins:
    - name: 40-flannel.conf
      contents: |
        [Service]
        EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
    - name: 60-dockeropts.conf
      contents: |
        [Service]
        Environment="DOCKER_OPTS=--log-opt=max-file=2 --log-opt=max-size=50m"
        Environment=DOCKER_SELINUX=

  - name: meta-data-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/meta-data-iptables.sh

      [Install]
      WantedBy=multi-user.target

  - name: dns-conntrack-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/dns-conntrack-iptables

      [Install]
      WantedBy=multi-user.target

  - name: dockercfg.service
    enable: true
    contents: |
      [Unit]
      After=network.target

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5
      ExecStartPre=/usr/bin/mkdir -p /root/.docker
      ExecStart=/opt/bin/dockercfg.sh

      [Install]
      WantedBy=multi-user.target

  - name: timesyncd-enable-network-time.service
    enable: true
    contents: |
      [Service]
      Type=oneshot
      ExecStart=/usr/bin/timedatectl set-ntp true

      [Install]
      WantedBy=multi-user.target

  - name: private-ipv4.service
    enable: true
    contents: |
      [Unit]
      After=network.target
      Description=Set PRIVATE_EC2_IPV4 env

      [Service]
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/systemctl set-environment PRIVATE_EC2_IPV4=$(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-ipv4)"
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

  - name: collect-instance-metadata.service
    enable: true
    contents: |
      [Unit]
      After=docker.service dockercfg.service private-ipv4.service

      [Service]
      Type=oneshot
      ExecStart=/opt/bin/collect-instance-metadata
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

  - name: kubelet.service
    enable: true
    contents: |
      [Unit]
      After=docker.service dockercfg.service meta-data-iptables.service private-ipv4.service collect-instance-metadata.service

      [Service]
      Environment=KUBELET_IMAGE_TAG=v1.13.5
      Environment=KUBELET_IMAGE_ARGS=--exec=/kube-component
      Environment=KUBELET_IMAGE_URL=docker://registry.opensource.zalan.do/teapot/kubelet
      Environment="RKT_RUN_ARGS=--insecure-options=image \
      --uuid-file-save=/var/run/kubelet-pod.uuid \
      --volume dns,kind=host,source=/etc/resolv.conf \
      --mount volume=dns,target=/etc/resolv.conf \
      --volume hosts,kind=host,source=/etc/hosts \
      --mount volume=hosts,target=/etc/hosts \
      --volume var-log,kind=host,source=/var/log \
      --mount volume=var-log,target=/var/log \
      --volume var-lib-cni,kind=host,source=/var/lib/cni \
      --mount volume=var-lib-cni,target=/var/lib/cni \
      --volume data,kind=host,source=/data \
      --mount volume=data,target=/data \
      --volume dockercfg,kind=host,source=/root/.docker/config.json \
      --mount volume=dockercfg,target=/root/.docker/config.json \
      --set-env=HOME=/root"
      EnvironmentFile=/var/run/kubelet.vars
      ExecStartPre=/usr/bin/mkdir -p /var/log/containers
      ExecStartPre=/bin/mkdir -p /var/lib/cni
      ExecStartPre=/bin/mkdir -p /data
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
      ExecStart=/usr/lib/coreos/kubelet-wrapper \
      --cni-conf-dir=/etc/kubernetes/cni/net.d \
      --network-plugin=cni \
      --container-runtime=docker \
      --register-with-taints={{if index .NodePool.ConfigItems "taints"}}{{.NodePool.ConfigItems.taints}}{{end}} \
      --register-node \
      --node-labels=kubernetes.io/role=worker \
      --node-labels=kubernetes.io/node-pool={{ .NodePool.Name }}{{if index .NodePool.ConfigItems "labels"}},{{.NodePool.ConfigItems.labels}}{{end}} \
      --node-labels=${SPOT_LABEL} \
{{- if eq .Cluster.ConfigItems.node_update_prepare_replacement_node "true" }}
      --node-labels=${REPLACEMENT_STRATEGY_LABEL} \
{{- end }}
      --node-labels={{ .Values.node_labels }} \
      --cluster-dns=${PRIVATE_EC2_IPV4} \
      --cluster-domain=cluster.local \
      --kubeconfig=/etc/kubernetes/kubeconfig \
      --healthz-bind-address=0.0.0.0 \
      --healthz-port=10248 \
      --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
      --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem \
      --cloud-provider=aws \
      --feature-gates=TaintBasedEvictions=true \
      --pod-infra-container-image=registry.opensource.zalan.do/teapot/pause-amd64:3.1 \
{{- if not (index .Cluster.ConfigItems "enable_cfs_quota") }}
      --cpu-cfs-quota=false \
{{- end }}
      --system-reserved=cpu=100m,memory=164Mi \
      --kube-reserved=cpu=100m,memory=282Mi
      ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  - name: kube-node-drainer.service
    enable: true
    contents: |
      [Unit]
      Description=drain this k8s node to make running pods time to gracefully shut down before stopping kubelet
      After=docker.service kubelet.service

      [Service]
      Type=oneshot
      RemainAfterExit=true
      ExecStart=/bin/true
      TimeoutStopSec=120s
      ExecStop=/opt/bin/drain-node

      [Install]
      WantedBy=multi-user.target

  - name: spot-termination-handler.service
    enable: true
    contents: |
      [Unit]
      Description=poll for AWS Spot termination signal and force node shutdown
      After=network.target collect-instance-metadata.service
      ConditionPathExists=/var/run/spot-instance

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5
      ExecStart=/usr/bin/rkt run --insecure-options=image \
        --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
        --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        docker://registry.opensource.zalan.do/teapot/spot-termination-handler:master-1
      ExecStopPost=-/opt/bin/spot-shutdown

      [Install]
      WantedBy=multi-user.target

storage:
  files:
  - filesystem: root
    path: /etc/kubernetes/kubeconfig
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: {{ .Cluster.APIServerURL }}
        users:
        - name: kubelet
          user:
            token: {{ .Cluster.ConfigItems.worker_shared_secret }}
        contexts:
        - context:
            cluster: local
            user: kubelet
          name: kubelet-context
        current-context: kubelet-context

  - filesystem: root
    path: /etc/kubernetes/cni/docker_opts_cni.env
    mode: 0644
    contents:
      inline: |
        DOCKER_OPT_BIP=""
        DOCKER_OPT_IPMASQ=""

  - filesystem: root
    path: /etc/kubernetes/ssl/worker.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/worker-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_key_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/ca.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.ca_cert_decompressed }}"

  - filesystem: root
    path: /opt/bin/dockercfg.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        iid="instance-identity-document:$(curl --silent --fail http://169.254.169.254/latest/dynamic/instance-identity/pkcs7)"
        iid="$(echo -n "$iid" | base64 -w 0)"
        cat << EOF > /root/.docker/config.json
        {
          "auths": {
            "https://pierone.stups.zalan.do": {
              "auth": "${iid}"
            }
          }
        }
        EOF

  - filesystem: root
    path: /opt/bin/meta-data-iptables.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables \
          --append PREROUTING \
          --protocol tcp \
          --destination 169.254.169.254 \
          --dport 80 \
          --in-interface cni0 \
          --match tcp \
          --jump DNAT \
          --table nat \
          --to-destination ${PRIVATE_EC2_IPV4}:8181

  - filesystem: root
    path: /opt/bin/dns-conntrack-iptables
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables -I PREROUTING 1 -t raw -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I PREROUTING 1 -t raw -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK

        /usr/sbin/iptables -I INPUT 1 -t filter -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I INPUT 1 -t filter -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT

  {{if index .Cluster.ConfigItems "docker-1.12"}}
  - filesystem: root
    path: /etc/coreos/docker-1.12
    mode: 0644
    contents:
      inline: yes
  {{end}}

  - filesystem: root
    path: /home/core/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /root/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /etc/systemd/timesyncd.conf
    mode: 0644
    contents:
      inline: |
        [Time]
        NTP=169.254.169.123

  - filesystem: root
    path: /etc/ssh/sshd_config
    mode: 0400
    contents:
      inline: |
        Subsystem sftp internal-sftp
        ClientAliveInterval 180
        AuthenticationMethods publickey
        UseDNS no
        UsePAM yes
        PrintLastLog no # handled by PAM
        PrintMotd no    # handled by PAM

  - filesystem: root
    path: /etc/sysctl.d/01-max-user-watches.conf
    mode: 0644
    contents:
      inline: |
        fs.inotify.max_user_watches=100000

  - filesystem: root
    path: /opt/bin/drain-node
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          --net=host \
          docker://registry.opensource.zalan.do/teapot/kubectl:v1.13.5 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          label node "$(hostname)" \
          lifecycle-status=draining \
          --overwrite

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --net=host \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          docker://registry.opensource.zalan.do/teapot/kubectl:v1.13.5 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          drain "$(hostname)" \
          --ignore-daemonsets \
          --delete-local-data \
          --force

  - filesystem: root
    path: /opt/bin/collect-instance-metadata
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        IS_SPOT="$(docker run --rm -i registry.opensource.zalan.do/teapot/is-spot-instance:master-1)"
        REPLACEMENT_STRATEGY="prepare-replacement"

        if [[ "${IS_SPOT}" == "true" ]]; then
          touch /var/run/spot-instance
          REPLACEMENT_STRATEGY="none"
        fi
        echo "REPLACEMENT_STRATEGY_LABEL=cluster-lifecycle-controller.zalan.do/replacement-strategy=${REPLACEMENT_STRATEGY}" >> /var/run/kubelet.vars
        echo "SPOT_LABEL=aws.amazon.com/spot=${IS_SPOT}" >> /var/run/kubelet.vars

  - filesystem: root
    path: /opt/bin/spot-shutdown
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        if [[ $EXIT_CODE -ne 0 ]]; then
          shutdown now
        fi

{{if gt (len .Values.instance_info.InstanceStorageDevices) 0 }}
  filesystems:
  - name: instance-storage
    mount:
      device: {{(index .Values.instance_info.InstanceStorageDevices 0).Path}}
      format: ext4
      wipe_filesystem: true
{{end}}
