# container linux config
passwd:
  users:
  - name: core
    ssh_authorized_keys:
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAt8djQXfn5U7H85oDzuZRfRONF+jVqn3Mp9t3tnBrJdKyTccfDovq1sekzEdOFdmj74yfS8bzaIO9pDUczy6j5k2MtDCoRnzAO6KZc46jMJ2GjhLArUuHLjmAw2r9LotZ30LEIEvJdmUI7mDlPMczv381PghyM8+DsYv62UrfjDiOsZ4EVkYnztQlO3ntNE16RFNj4fbfErQz67kmw0lB8C6bAf0RuRmvXzB7xRMplmknQnLusoURmySKdZM0GUe0VY6fmqOsgzHVLoEs1m82V8QK1ac/1DSHA91v50MbpCjTVLaRhjR8nmhWBedlhb6j5ClZdAQ8iwyyWC1MFQuvKw== henning@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICZh6WTi6o6ofWqJBmJjzYZdM/La5B/aZQLfpuho6rx7 mikkel.larsen@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIN0w9xqF0Iu8L+o3NKaw0HZ5m7I0cANfVDZxE/rjfdeH martin.linkhorst@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFVmr+Y3hBvnz2xgSWoaxaPxnExcEXOghUozupU9bKNR nick@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCq3oEP8qhMvGtlR1bgVc9tFOVJ5B5RMKtm6UQ/zXQUpm8DQ04SxdM2U7TfuLien2HSfpHAlYe0eLJZUfIqCXUeZ37v0ozj2RglireEcJm0t9XJ7kTS4kqVxrL6iuN6qQVGHs0vxoo/o9+SP0YkuuJoXwJvVI4yKVbnbfA5hKaAffAYPmfgqOZ7+3AMwmaj/D3tI0xVEA48ptGkj5nnOl0pXlfLRNvbnXOCa/dTKUgkma1F0lXoTipkRspsMEiAnwfJ1dwnzgNzllt//Ao/H+yOVR8fWJ7d+nowszIk6zwUR7c6walxKKf5Oy5bQBU49MZ6xLP1oma9F2+llmV7qpqx rodrigo.reis@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIABg+lm8vgo9sKRjYb4DV3gmuoaWXuFyjb3Kb+wnCz+A sandor.szuecs@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINkh52Py+FvH9CRLDQg0gzvjEIrzwA45yMTXTsl2BVxV alexey.ermakov@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDakbukcXXmMyyxw5h3J8h41FK8b8/qjiX/u4BP+qQpQ3x/1VkppIpzHwLcjVThNspwOKKU6QwcaLKpJ+g3mYWdilsl7eFT5hAiIXq4OLe9XynXp1ZEduqKk7tgvjClo7OzuSdEfKtWkD35hzXWigVfJFZhSN9bLdiEzuIEHf0jEo2mUXADgAzSwbaTcVUreH/IR6SAYBVh4XgKoQjOmWrizJguYNk+DgBW6/R+5JQi8YYPSuevbcEKXXm1ZfOudNv2ebOL1rA4I+VDe2xW+JVwKIvAN3odt0zOH2NFr1kFU1GTVyD2ZYtyTNNae9a+mQ2Ltw5pzKjA4zjFctQrrI+yPEIcf64mccumIoRMjWozPB801VbZqUVyG0XhzHW7RNmTmmLsNa/STtYI9Xaqfrz2n3PgpYJ/bB42l4Ez9Oblh9rZU0aisr6risLh5XXgCa9NezMjTczMHJE0jVINgeIMsCleNF0HrkF26Uo1MJO1pOMZOGZzXSksB79TmTwjRdvQT/uvSAElBrthmRgj82pDsRdNoipp6VMx54m2KbvRyq79PVZP69NGWdAhJBw1SV0447kT1tuoqDTUskS4es1GoSu5sqtW4eF4Q6oju8+01l3ygLKmPPTbFVRcs6bBvHjnB3RYwgai5z0U16rsh7LKUwhYsRiar3lmrhVvoc95sw== muhammad.muaaz.saleem@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKIX5twzJYryUz6bw/0SLJRBg5RNfxUZiXOZ/1E75c2r arjun.naik@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC6QTMJEdUHX/50ZzwelLGouD9mXpeHyTz0FW/vK1NPQHE0OM193bCXy0A+vjf7GaMUq8KOB8bdm96w3ffGM7ZKvyMOh4UHsxojabVZyKCaDafpQhzVSn1CRVy7vyJKoWkE5usuEbD2dKLxY5Svpw78+DHSdjNGTyFXPf/YGqNP2xrjTP96sx8yWg02w49kD3XrBQaFg5YicCHAAAwQ0IObx7dlT+6B9QrAuVY1nhIq1yqmKvLZa6h6MkoQPlp3Y3ksBhs0C+RNpB51EbBw2VMZJ+rbk/pOKDhlOMSDrLJb6TpAo6P7j1lvK0NrKpTrsifNPaJ5GtILa1hjVkMXpxJNDjc6+ApOzIuKhaACaufV/C7yKd1bGYFClkoDqxYck8mgQnBA+MAYWgLr4Bxu/R0Q1JwPiVDd6QGJcMSiBayIR5GT47X8SeV2yrNLBzbc7J3hctYNQxiNc+H2pxLr9P+yFCB6Xql0poFaG8vppAETPxeF1CaY86WsknPA5SqnDipr1O6OLJWcQMWw9iHj9mDMwyNpXJK3T5k/aoE+URZb2GxnTRzzcC3H+yi8ol3NsbyylHTSGwmbfbupzSenh2sSq7aZipDX6ELm0RbuogIbalOAtOUROGiG0HNcG8GbJLky6qMBSyh59VnaCDWPfFde1gsg+SH8Od3y2TOA+1rtbw== jannis.rake-revelant@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/4JrWOoGJWI2pDKDGFo16o7N9OzbN0aRpxzjQS78unkCSrt/5wv3Vc8lZmd30V7To1wH08zVdaXCIKiEfUJFsHfwg+3ASYOe8kKuSFcTYfMSV1quFZB4JCI+2+75rfYP+ra0RI18+yZmvNBN+74U4t+YEqnjgtLCbq76ePdK719IYjw7haGNwzltLgg3vcqyob0NUGs4mzccEGH3ZQ0TwW60NhmdxR+IdgZECc2l45xYGrF4Ayis4InSVg+0hhNVBH0lTpXHQj+AX0eafO5qPNISRIZ+LDkBast7rZdRvK1kGLUZoVrOblrIL6yixjfXMci5hcDopGcOyMkoIOzCp arpad.ryszka@zalando.de'
systemd:
  units:
  # disable automatic updates
  - name: update-engine.service
    mask: true
  - name: locksmithd.service
    mask: true

  # disable socket activation for sshd
  - name: sshd.service
    enable: true
  - name: sshd.socket
    mask: true

  - name: set-hostname.service
    enable: true
    contents: |
      [Unit]
      Wants=network.target
      Before=docker.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/hostnamectl set-hostname $(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-hostname)"

      [Install]
      WantedBy=multi-user.target

  - name: update-system-settings.service
    enable: true
    contents: |
      [Unit]
      Before=docker.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStartPre=/bin/sh -c 'test -f /sys/module/nvme_core/parameters/io_timeout && /usr/bin/echo 4294967295 > /sys/module/nvme_core/parameters/io_timeout || true'
      ExecStart=/bin/sh -c '/usr/bin/echo madvise > /sys/kernel/mm/transparent_hugepage/enabled'

      [Install]
      WantedBy=multi-user.target

{{ if ne .Cluster.ConfigItems.etcd_proxy_as_sidecar "true" }}
  - name: etcd-member.service
    enable: true
    contents: |
      [Unit]
      Wants=network.target

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5s
      StartLimitIntervalSec=0
      ExecStartPre=/usr/bin/mkdir --parents /var/lib/coreos
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/lib/coreos/etcd-member-wrapper.uuid
      ExecStart=/usr/bin/rkt run --uuid-file-save=/var/lib/coreos/etcd-member-wrapper.uuid --port=2379-tcp:2379 --mount volume=dns,target=/etc/resolv.conf --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true --insecure-options=image docker://registry.opensource.zalan.do/teapot/etcd-proxy:master-2 -- {{ .Cluster.ConfigItems.etcd_endpoints }}
      ExecStop=-/usr/bin/rkt stop --uuid-file=/var/lib/coreos/etcd-member-wrapper.uuid

      [Install]
      WantedBy=multi-user.target
{{ end }}

  - name: docker.service
    dropins:
    - name: 40-flannel.conf
      contents: |
        [Service]
        EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
    - name: 60-dockeropts.conf
      contents: |
        [Service]
        Environment="DOCKER_OPTS=--log-opt=max-file=2 --log-opt=max-size=50m"
        Environment=DOCKER_SELINUX=

  - name: meta-data-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/meta-data-iptables.sh

      [Install]
      WantedBy=multi-user.target

  - name: dns-conntrack-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/dns-conntrack-iptables

      [Install]
      WantedBy=multi-user.target

  - name: dockercfg.service
    enable: true
    contents: |
      [Unit]
      After=network.target

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5
      ExecStartPre=/usr/bin/mkdir -p /root/.docker
      ExecStart=/opt/bin/dockercfg.sh

      [Install]
      WantedBy=multi-user.target

  - name: timesyncd-enable-network-time.service
    enable: true
    contents: |
      [Service]
      Type=oneshot
      ExecStart=/usr/bin/timedatectl set-ntp true

      [Install]
      WantedBy=multi-user.target

  - name: gen-controller-manager-config.service
    enable: true
    contents: |
      [Service]
      Type=oneshot
      ExecStart=/opt/bin/gen-controller-manager-config.sh

      [Install]
      WantedBy=multi-user.target

  - name: private-ipv4.service
    enable: true
    contents: |
      [Unit]
      After=network.target
      Description=Set PRIVATE_EC2_IPV4 env

      [Service]
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/systemctl set-environment PRIVATE_EC2_IPV4=$(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-ipv4)"
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

  - name: kubelet.service
    enable: true
    contents: |
      [Unit]
      After=docker.service dockercfg.service meta-data-iptables.service private-ipv4.service

      [Service]
      Environment=KUBELET_IMAGE_TAG=v1.13.7
      Environment=KUBELET_IMAGE_ARGS=--exec=/kubelet
      Environment=KUBELET_IMAGE_URL=docker://registry.opensource.zalan.do/teapot/kubelet
      Environment="RKT_RUN_ARGS=--insecure-options=image \
      --uuid-file-save=/var/run/kubelet-pod.uuid \
      --volume dns,kind=host,source=/etc/resolv.conf \
      --mount volume=dns,target=/etc/resolv.conf \
      --volume var-log,kind=host,source=/var/log \
      --mount volume=var-log,target=/var/log \
      --volume var-lib-cni,kind=host,source=/var/lib/cni \
      --mount volume=var-lib-cni,target=/var/lib/cni \
      --volume dockercfg,kind=host,source=/root/.docker/config.json \
      --mount volume=dockercfg,target=/root/.docker/config.json \
      --set-env=HOME=/root"
      ExecStartPre=/usr/bin/mkdir -p /var/log/containers
      ExecStartPre=/bin/mkdir -p /var/lib/cni
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
      ExecStart=/usr/lib/coreos/kubelet-wrapper \
      --cni-conf-dir=/etc/kubernetes/cni/net.d \
      --network-plugin=cni \
      --container-runtime=docker \
      --register-with-taints=node-role.kubernetes.io/master=:NoSchedule{{if index .NodePool.ConfigItems "taints"}},{{.NodePool.ConfigItems.taints}}{{end}} \
      --node-labels=node-role.kubernetes.io/master,kubernetes.io/role=master,master=true \
      --node-labels=cluster-lifecycle-controller.zalan.do/decommission-priority=999 \
      --node-labels=cluster-lifecycle-controller.zalan.do/replacement-strategy=ensure-minimum-healthy-nodes \
      --node-labels=kubernetes.io/node-pool={{ .NodePool.Name }}{{if index .NodePool.ConfigItems "labels"}},{{.NodePool.ConfigItems.labels}}{{end}} \
      --node-labels={{ .Values.node_labels }} \
      --cluster-dns=${PRIVATE_EC2_IPV4} \
      --kubeconfig=/etc/kubernetes/kubeconfig \
      --cloud-provider=aws \
      --pod-infra-container-image=registry.opensource.zalan.do/teapot/pause-amd64:3.1 \
      --config=/etc/kubernetes/config/kubelet.yaml
      ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  - name: kube-node-drainer.service
    enable: true
    contents: |
      [Unit]
      Description=drain this k8s node to make running pods time to gracefully shut down before stopping kubelet
      After=docker.service kubelet.service

      [Service]
      Type=oneshot
      RemainAfterExit=true
      ExecStart=/bin/true
      TimeoutStopSec=120s
      ExecStop=/opt/bin/drain-node

      [Install]
      WantedBy=multi-user.target

storage:
  files:
  - filesystem: root
    path: /etc/kubernetes/kubeconfig
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: http://127.0.0.1:8080
        users:
        - name: kubelet
        contexts:
        - context:
            cluster: local
            user: kubelet

  - filesystem: root
    path: /etc/kubernetes/config/kubelet.yaml
    mode: 0644
    contents:
      inline: |
        # https://github.com/kubernetes/kubernetes/blob/v1.13.6/staging/src/k8s.io/kubelet/config/v1beta1/types.go
        apiVersion: kubelet.config.k8s.io/v1beta1
        kind: KubeletConfiguration
        staticPodPath: "/etc/kubernetes/manifests"
        clusterDomain: cluster.local
{{- if not (index .Cluster.ConfigItems "enable_cfs_quota") }}
        cpuCFSQuota: false
{{- end }}
{{- if ne .NodePool.ConfigItems.pod_max_pids "-1" }}
        featureGates:
          SupportPodPidsLimit: true
        podPidsLimit: {{ .NodePool.ConfigItems.pod_max_pids }}
{{- end }}
        maxPods: {{ .Cluster.ConfigItems.node_max_pods }}
        healthzPort: 10248
        healthzBindAddress: "0.0.0.0"
        tlsCertFile: "/etc/kubernetes/ssl/worker.pem"
        tlsPrivateKeyFile: "/etc/kubernetes/ssl/worker-key.pem"
        eventRecordQPS: 50
        eventBurst: 50
        kubeAPIQPS: 50
        kubeAPIBurst: 50
        systemReserved:
          cpu: "100m"
          memory: "164Mi"
        kubeReserved:
          cpu: "100m"
          memory: "282Mi"
        authentication:
          anonymous:
            enabled: false
          webhook:
            enabled: true
            cacheTTL: "2m"
          x509:
            clientCAFile: "/etc/kubernetes/ssl/ca.pem"
        authorization:
          mode: Webhook
          webhook:
            cacheAuthorizedTTL: "5m"
            cacheUnauthorizedTTL: "30s"

  - filesystem: root
    path: /etc/kubernetes/config/authn.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: authz-webhook
            cluster:
              server: http://127.0.0.1:8081/authentication
        users:
          - name: authz-webhook-client
            user:
              token: notused
        current-context: authz-webhook
        contexts:
        - context:
            cluster: authz-webhook
            user: authz-webhook-client
          name: authz-webhook

  - filesystem: root
    path: /etc/kubernetes/config/authz.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: authz-webhook
            cluster:
              server: http://127.0.0.1:8081/authorization
        users:
          - name: authz-webhook-client
        current-context: authz-webhook
        contexts:
        - context:
            cluster: authz-webhook
            user: authz-webhook-client
          name: authz-webhook

  - filesystem: root
    path: /etc/kubernetes/cni/docker_opts_cni.env
    mode: 0644
    contents:
      inline: |
        DOCKER_OPT_BIP=""
        DOCKER_OPT_IPMASQ=""

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-apiserver
          namespace: kube-system
          labels:
            application: kube-apiserver
            version: v1.13.7
          annotations:
            kubernetes-log-watcher/scalyr-parser: |
              [{"container": "webhook", "parser": "json-structured-log"}]
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          hostNetwork: true
          containers:
          - name: kube-apiserver
            image: registry.opensource.zalan.do/teapot/kube-apiserver:v1.13.7
            args:
            - --apiserver-count={{ .Values.apiserver_count }}
            - --bind-address=0.0.0.0
            - --insecure-bind-address=0.0.0.0
            - --etcd-servers=http://127.0.0.1:2379
            - --etcd-prefix={{ .Cluster.ConfigItems.apiserver_etcd_prefix }}
            - --storage-backend=etcd3
            - --storage-media-type=application/vnd.kubernetes.protobuf
            - --allow-privileged=true
            - --service-cluster-ip-range=10.3.0.0/16
            - --secure-port=443
            - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodSecurityPolicy,ImagePolicyWebhook,Priority,ExtendedResourceToleration
            - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
            - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --service-account-key-file=/etc/kubernetes/ssl/service-account-public-key.pem
            - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1=true,policy/v1beta1/podsecuritypolicy=true,imagepolicy.k8s.io/v1alpha1=true,authorization.k8s.io/v1beta1=true,scheduling.k8s.io/v1alpha1=true,admissionregistration.k8s.io/v1beta1=true
            - --authentication-token-webhook-config-file=/etc/kubernetes/config/authn.yaml
            - --authentication-token-webhook-cache-ttl=10s
            - --cloud-provider=aws
            - --authorization-mode=Webhook,RBAC
            - --authorization-webhook-config-file=/etc/kubernetes/config/authz.yaml
            - --admission-control-config-file=/etc/kubernetes/config/image-policy-webhook.yaml
            - --feature-gates=TaintNodesByCondition={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}},ScheduleDaemonSetPods={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}},TTLAfterFinished=true,CustomResourceWebhookConversion={{.Cluster.ConfigItems.custom_resource_webhook_conversion}}
            - --anonymous-auth=false
            {{ if or (eq .Cluster.Environment "production") (index .Cluster.ConfigItems "audittrail_url") }}
            - --audit-webhook-config-file=/etc/kubernetes/config/audit.yaml
            - --audit-webhook-mode=batch
            - --audit-webhook-version=audit.k8s.io/v1beta1
            - --audit-policy-file=/etc/kubernetes/config/audit-policy.yaml
            {{ end }}
            {{ if eq .Cluster.Environment "e2e" }}
            - --audit-log-path=/var/log/kube-audit.log
            - --audit-log-maxage=0
            - --audit-log-maxbackup=0
            - --audit-policy-file=/etc/kubernetes/config/audit-policy.yaml
            {{ end }}
            # enable aggregated apiservers
            - --client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --requestheader-allowed-names=aggregator
            - --requestheader-extra-headers-prefix=X-Remote-Extra-
            - --requestheader-group-headers=X-Remote-Group
            - --requestheader-username-headers=X-Remote-User
            - --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem
            - --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem
            # kubelet authentication
            - --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem
            - --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-client.pem
            - --kubelet-client-key=/etc/kubernetes/ssl/kubelet-client-key.pem
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                port: 8080
                path: /healthz
              initialDelaySeconds: 120
              timeoutSeconds: 15
            ports:
            - containerPort: 443
              hostPort: 443
              name: https
            - containerPort: 8080
              hostPort: 8080
              name: local
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/kubernetes/config
              name: kubernetes-configs
              readOnly: true
            resources:
              requests:
                cpu: 100m
                memory: 200Mi
          - image: registry.opensource.zalan.do/teapot/admission-controller:master-28
            name: admission-controller
            readinessProbe:
              httpGet:
                scheme: HTTPS
                path: /healthz
                port: 8085
              initialDelaySeconds: 5
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 50m
                memory: 100Mi
            args:
              - --address=:8085
              - --master=http://127.0.0.1:8080
              - --tls-cert-file=/etc/kubernetes/ssl/admission-controller.pem
              - --tls-key-file=/etc/kubernetes/ssl/admission-controller-key.pem
              - --pod-default-cpu-request={{ .Cluster.ConfigItems.teapot_admission_controller_default_cpu_request }}
              - --pod-default-memory-request={{ .Cluster.ConfigItems.teapot_admission_controller_default_memory_request }}
              - --pod-ignore-namespaces={{ .Cluster.ConfigItems.teapot_admission_controller_ignore_namespaces }}
              - --pod-dns-ndots={{ .Cluster.ConfigItems.teapot_admission_controller_ndots }}
{{- if eq .Cluster.ConfigItems.teapot_admission_controller_process_resources "true" }}
              - --pod-process-resources
{{- end }}
{{- if eq .Cluster.ConfigItems.teapot_admission_controller_inject_environment_variables "true" }}
              - --pod-enable-env-inject
              - --pod-inject-env=_PLATFORM_ACCOUNT={{ .Cluster.Alias }}
              - --pod-inject-env=_PLATFORM_OPENTRACING_TAG_ACCOUNT={{ .Cluster.Alias }}
              - --pod-inject-env=_PLATFORM_OPENTRACING_LIGHTSTEP_COLLECTOR_PORT=8443
              - --pod-inject-env=_PLATFORM_OPENTRACING_LIGHTSTEP_COLLECTOR_HOST=tracing.stups.zalan.do
              - --pod-inject-env=_PLATFORM_OPENTRACING_LIGHTSTEP_ACCESS_TOKEN={{ .ConfigItems.lightstep_token }}
{{- if eq .Cluster.Environment "e2e" }}
              - --pod-inject-env=_PLATFORM_E2E=injected
{{- end }}
{{- end }}
{{- if eq .Cluster.ConfigItems.experimental_schedule_daemonset_pods "true" }}
              - --node-add-not-ready-taint
{{- end }}
{{- if eq .Cluster.ConfigItems.teapot_admission_controller_validate_application_label "true" }}
              - --validate-application-label
              - --validate-application-label-min-creation-time={{ .Cluster.ConfigItems.teapot_admission_controller_application_min_creation_time }}
              - --application-registry-url=http://127.0.0.1:8285/
{{- end }}
              - --resource-cutoff-timestamp={{ .Cluster.ConfigItems.teapot_admission_controller_resource_cutoff_timestamp }}
            ports:
              - containerPort: 8085
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
          - image: registry.opensource.zalan.do/teapot/k8s-authnz-webhook:v0.5.8
            name: webhook
            ports:
            - containerPort: 8081
            livenessProbe:
              httpGet:
                path: /healthcheck
                port: 8081
              initialDelaySeconds: 30
              timeoutSeconds: 5
            readinessProbe:
              httpGet:
                path: /healthcheck
                port: 8081
              initialDelaySeconds: 5
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 50m
                memory: 50Mi
            args:
              - --tokens-file=/etc/kubernetes/config/tokenfile.csv
            env:
              - name: TEAMS_API_URL
                value: https://teams.auth.zalando.com
              - name: CLUSTER_ID
                value: {{if index .Cluster.ConfigItems "webhook_id"}}{{ .Cluster.ConfigItems.webhook_id }}{{else}}{{ .Cluster.ID }}{{end}}
              - name: TOKEN_INTROSPECTION_URL
                value: http://127.0.0.1:{{ if eq .Cluster.Environment "production" }}9021{{else}}9023{{end}}/oauth2/introspect
              - name: USER_GROUPS
                value: kubelet=system:masters,stups_cluster-lifecycle-manager=system:masters
              - name: BUSINESS_PARTNER_IDS
                value: {{ .Cluster.ConfigItems.apiserver_business_partner_ids }}
            volumeMounts:
            - mountPath: /etc/kubernetes/config
              name: kubernetes-configs
              readOnly: true
          - image: registry.opensource.zalan.do/foundation/platform-iam-tokeninfo:2fca26c
            name: tokeninfo
            ports:
              - containerPort: 9021
            livenessProbe:
              httpGet:
                path: /health
                port: 9021
              periodSeconds: 3
              failureThreshold: 2
            readinessProbe:
              httpGet:
                path: /health
                port: 9021
              periodSeconds: 3
              failureThreshold: 2
            resources:
              requests:
                cpu: 50m
                memory: 20Mi
            env:
              - name: OPENID_PROVIDER_CONFIGURATION_URL
                value: https://identity.zalando.com/.well-known/openid-configuration
              - name: ENABLE_INTROSPECTION
                value: "true"
{{ if ne .Cluster.Environment "production" }}
          - name: tokeninfo-sandbox
            image: registry.opensource.zalan.do/foundation/platform-iam-tokeninfo:2fca26c
            ports:
            - containerPort: 9022
            livenessProbe:
              httpGet:
                path: /health
                port: 9022
              periodSeconds: 3
              failureThreshold: 2
            readinessProbe:
              httpGet:
                path: /health
                port: 9022
              periodSeconds: 3
              failureThreshold: 2
            resources:
              requests:
                cpu: 50m
                memory: 20Mi
            env:
            - name: OPENID_PROVIDER_CONFIGURATION_URL
              value: https://sandbox.identity.zalando.com/.well-known/openid-configuration
            - name: ENABLE_INTROSPECTION
              value: "true"
            - name: ISSUER
              value: "https://sandbox.identity.zalando.com"
            - name: LISTEN_ADDRESS
              value: ":9022"
          - name: skipper-tokeninfo-bridge
            image: registry.opensource.zalan.do/pathfinder/skipper:v0.10.243
            args:
            - skipper
            - -address=:9023
            - -inline-routes
            - |
              health: Path("/healthz") -> inlineContent("ok") -> <shunt>;
              q2h: Path("/oauth2/introspect") && QueryParam("token")
                -> queryToHeader("token", "Authorization", "Bearer %s")
                -> setRequestHeader("X-Checking-JWT", "true")
                -> <loopback>;
              prod: Path("/oauth2/introspect") && QueryParam("token") && Header("X-Checking-JWT", "true") && JWTPayloadAllKV("iss", "https://identity.zalando.com")
                -> dropRequestHeader("Authorization") -> dropRequestHeader("X-Checking-JWT")
                -> "http://127.0.0.1:9021/oauth2/introspect";
              sandbox: Path("/oauth2/introspect") && QueryParam("token") && Header("X-Checking-JWT", "true") && JWTPayloadAllKV("iss", "https://sandbox.identity.zalando.com")
                -> dropRequestHeader("Authorization") -> dropRequestHeader("X-Checking-JWT")
                -> "http://127.0.0.1:9022/oauth2/introspect";
              fallback1: Path("/oauth2/introspect") && QueryParam("token") && Header("X-Checking-JWT", "true")
                -> inlineContent("{\"active\":false}") -> <shunt>;
              fallback2: Path("/oauth2/introspect")
                -> inlineContent("{\"active\":false}") -> <shunt>;
            ports:
            - containerPort: 9023
            readinessProbe:
              httpGet:
                path: /healthz
                port: 9023
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 100m
                memory: 20Mi
{{ end }}
          - image: registry.opensource.zalan.do/teapot/image-policy-webhook:0.5.3
            name: image-policy-webhook
            args:
            - --policy={{ .Cluster.ConfigItems.image_policy }}
            - --failure-policy=fail
            ports:
            - containerPort: 8083
          - name: nginx
            image: registry.opensource.zalan.do/teapot/nginx:1.12.1
            ports:
            - containerPort: 8082
            resources:
              requests:
                cpu: 5m
                memory: 50Mi
            volumeMounts:
            - name: config-volume
              mountPath: /etc/nginx
          - name: skipper-proxy
            image: registry.opensource.zalan.do/pathfinder/skipper:v0.10.243
            args:
            - skipper
            - -address=:8443
            - -tls-cert=/etc/kubernetes/ssl/apiserver.pem
            - -tls-key=/etc/kubernetes/ssl/apiserver-key.pem
            - -insecure
            - -experimental-upgrade
            - -runtime-metrics
            - -enable-connection-metrics
            - -enable-prometheus-metrics
            - -write-timeout-server=60m
            - -inline-routes
            - |
              s: JWTPayloadAllKV("iss", "kubernetes/serviceaccount")
                -> enableAccessLog()
                -> {{ if eq .ConfigItems.allow_external_service_accounts "true" }}"https://127.0.0.1:443"{{ else }}status(401) -> <shunt>{{ end }};
              h: Path("/kube-system/healthz")
                -> setPath("/healthz")
                -> disableAccessLog()
                -> "http://127.0.0.1:8080";
              all: *
                -> disableAccessLog()
                -> "https://127.0.0.1:443";
            ports:
            - containerPort: 8443
            readinessProbe:
              httpGet:
                scheme: HTTPS
                path: /kube-system/healthz
                port: 8443
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 100m
                memory: 250Mi
            securityContext:
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1000
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
{{ if eq .Cluster.ConfigItems.etcd_proxy_as_sidecar "true"}}
          - name: etcd-proxy
            image: registry.opensource.zalan.do/teapot/etcd-proxy:master-3
            args:
            - {{ .Cluster.ConfigItems.etcd_endpoints }}
            ports:
            - containerPort: 2379
            resources:
              requests:
                cpu: 25m
                memory: 25Mi
{{ end }}
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /etc/kubernetes/config
            name: kubernetes-configs
          - hostPath:
              path: /etc/kubernetes/nginx
            name: config-volume

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-controller-manager
          namespace: kube-system
          labels:
            application: kube-controller-manager
            version: v1.13.7
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          containers:
          - name: kube-controller-manager
            image: registry.opensource.zalan.do/teapot/kube-controller-manager:v1.13.7
            args:
            - --kubeconfig=/etc/kubernetes/controller-kubeconfig
            - --leader-elect=true
            - --service-account-private-key-file=/etc/kubernetes/ssl/service-account-private-key.pem
            - --root-ca-file=/etc/kubernetes/ssl/ca.pem
            - --cloud-provider=aws
            - --cloud-config=/etc/kubernetes/cloud-config.ini
            - --feature-gates=TaintNodesByCondition={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}},ScheduleDaemonSetPods={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}},TTLAfterFinished=true
            - --horizontal-pod-autoscaler-use-rest-clients=true
            - --use-service-account-credentials=true
            - --configure-cloud-routes=false
            - --allocate-node-cidrs=true
            - --cluster-cidr=10.2.0.0/16
            - --node-cidr-mask-size={{ .Cluster.ConfigItems.node_cidr_mask_size }}
            - --terminated-pod-gc-threshold=500
            - --kube-api-qps=50
            - --horizontal-pod-autoscaler-use-rest-clients=true
            - --horizontal-pod-autoscaler-downscale-delay={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_downscale_delay }}
            - --horizontal-pod-autoscaler-sync-period={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_sync_period }}
            - --horizontal-pod-autoscaler-tolerance={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_tolerance }}
            - --horizontal-pod-autoscaler-upscale-delay={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_upscale_delay }}
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 15
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/kubernetes
              name: kubeconfig
              readOnly: true
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /etc/kubernetes
            name: kubeconfig

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-scheduler.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-scheduler
          namespace: kube-system
          labels:
            application: kube-scheduler
            version: v1.13.7
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          hostNetwork: true
          containers:
          - name: kube-scheduler
            image: registry.opensource.zalan.do/teapot/kube-scheduler:v1.13.7
            args:
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            - --feature-gates=TaintBasedEvictions=true,TaintNodesByCondition={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}},ScheduleDaemonSetPods={{.Cluster.ConfigItems.experimental_schedule_daemonset_pods}}
            env:
            - name: KUBE_MAX_PD_VOLS
              value: "26"
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 15

  - filesystem: root
    path: /etc/kubernetes/nginx/nginx.conf
    mode: 0644
    contents:
      inline: |
        user nginx;
        worker_processes 1;
        error_log /dev/stdout info;
        events {
          worker_connections 1024;
        }
        http {
          default_type application/octet-stream;
          server_tokens off;
          access_log off;
          server {
            listen 8082;
            server_name _;
            location / {
              if ($request_method != GET) {
                return 403;
              }
              proxy_pass_request_body off;
              proxy_pass http://127.0.0.1:8080;
            }
            location ~ /secrets(/|$) {
              return 403 "unauthorized";
            }
          }
        }

  - filesystem: root
    path: /opt/bin/gen-controller-manager-config.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        token="$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')"
        cat << EOF > /etc/kubernetes/controller-kubeconfig
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: https://127.0.0.1
            certificate-authority: /etc/kubernetes/ssl/ca.pem
        contexts:
        - name: local
          context:
            cluster: local
            user: kube-controller-manager
        current-context: local
        users:
        - name: kube-controller-manager
          user:
            token: ${token}
        EOF

        echo "${token},system:kube-controller-manager,system:kube-controller-manager" >> /etc/kubernetes/config/tokenfile.csv

  - filesystem: root
    path: /etc/kubernetes/config/tokenfile.csv
    mode: 0644
    contents:
      inline: |
        {{ .Cluster.ConfigItems.worker_shared_secret }},kubelet,kubelet

  - filesystem: root
    path: /etc/kubernetes/config/image-policy-webhook.yaml
    mode: 0644
    contents:
      inline: |
        imagePolicy:
          kubeConfigFile: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
          allowTTL: 2
          denyTTL: 2
          retryBackoff: 500
          defaultAllow: false

  - filesystem: root
    path: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: image-policy-webhook
            cluster:
              server: http://127.0.0.1:8083/admit
        users:
          - name: image-policy-webhook
        current-context: image-policy-webhook
        contexts:
        - context:
            cluster: image-policy-webhook
            user: image-policy-webhook
          name: image-policy-webhook

  - filesystem: root
    path: /etc/kubernetes/config/audit.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: audit
          cluster:
            server: http://127.0.0.1:8889/audit
        contexts:
        - name: audit
          context:
            cluster: audit
            user: audit
        current-context: audit

  - filesystem: root
    path: /etc/kubernetes/config/audit-policy.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: audit.k8s.io/v1beta1
        kind: Policy
        rules:
          # The following requests were manually identified as high-volume and low-risk,
          # so drop them.
          - level: None
            users: ["system:kube-proxy"]
            verbs: ["watch"]
            resources:
              - group: "" # core
                resources: ["endpoints", "services", "services/status"]
          # don't audit any kubelet events
          # only enable it in e2e because we use kubelet as the identity for running e2e
          {{ if ne .Cluster.Environment "e2e" }}
          - level: None
            users: ["kubelet"] # legacy kubelet identity
          {{ end }}
          # don't audit events from the system:unsecured user. This is the user
          # used when connecting to the apiserver over localhost, and will
          # usuaully be done by the local kubelet.
          - level: None
            users: ["system:unsecured"]
          # don't audit events from the system:apiserver user
          - level: None
            users: ["system:apiserver"]
          {{ if eq .ConfigItems.audit_pod_events "true" }}
          # audit pod events
          - level: Request
            omitStages:
              - "RequestReceived"
            verbs: ["create", "delete", "update", "patch", "deletecollection"]
            resources:
              - group: "" # core
                resources: ["pods"]
          {{ end }}
          # don't audit any kube-controller-manager events
          - level: None
            users: ["system:kube-controller-manager"]
          # Don't audit events from system users in kube-system
          - level: None
            userGroups: ["system:serviceaccounts:kube-system"]
          # Don't audit events from high traffic infrastructure components
          - level: None
            users: ["credentials-provider"]
          - level: None
            userGroups: ["system:nodes"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["nodes", "nodes/status"]
          - level: None
            users:
              - system:kube-controller-manager
              - system:kube-scheduler
              - system:serviceaccount:kube-system:endpoint-controller
            verbs: ["get", "update"]
            namespaces: ["kube-system"]
            resources:
              - group: "" # core
                resources: ["endpoints"]
          - level: None
            users: ["system:apiserver"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
          # Don't log HPA fetching metrics.
          - level: None
            users:
              - system:kube-controller-manager
            verbs: ["get", "list"]
            resources:
              - group: "metrics.k8s.io"
          # Don't log these read-only URLs.
          - level: None
            nonResourceURLs:
              - /healthz*
              - /version
              - /swagger*
          # Don't log events requests.
          - level: None
            resources:
              - group: "" # core
                resources: ["events"]
          # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
          # so only log at the Metadata level.
          - level: Metadata
            resources:
              - group: "" # core
                resources: ["secrets", "configmaps"]
              - group: authentication.k8s.io
                resources: ["tokenreviews"]
            omitStages:
              - "RequestReceived"
          # Don't audit read-only events.
          - level: None
            verbs: ["watch", "list", "get"]
          # Default level for all other requests.
          - level: Request
            omitStages:
              - "RequestReceived"

  - filesystem: root
    path: /etc/kubernetes/ssl/ca.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.ca_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/apiserver.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.apiserver_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/apiserver-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.apiserver_key_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/worker.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/worker-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/proxy-client.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.proxy_client_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/proxy-client-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.proxy_client_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/kubelet-client.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.kubelet_client_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/kubelet-client-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.kubelet_client_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/admission-controller.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.admission_controller_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/admission-controller-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.admission_controller_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/service-account-private-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.service_account_private_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/service-account-public-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.service_account_private_key | base64Decode | publicKey | base64 }}"

  - filesystem: root
    path: /etc/kubernetes/cloud-config.ini
    mode: 0644
    contents:
      inline: |
        [global]
        DisableSecurityGroupIngress = true

  - filesystem: root
    path: /opt/bin/dockercfg.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        iid="instance-identity-document:$(curl --silent --fail http://169.254.169.254/latest/dynamic/instance-identity/pkcs7)"
        iid="$(echo -n "$iid" | base64 -w 0)"
        cat << EOF > /root/.docker/config.json
        {
          "auths": {
            "https://pierone.stups.zalan.do": {
              "auth": "${iid}"
            }
          }
        }
        EOF

  - filesystem: root
    path: /opt/bin/meta-data-iptables.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables \
          --append PREROUTING \
          --protocol tcp \
          --destination 169.254.169.254 \
          --dport 80 \
          --in-interface cni0 \
          --match tcp \
          --jump DNAT \
          --table nat \
          --to-destination ${PRIVATE_EC2_IPV4}:8181

  {{if index .Cluster.ConfigItems "docker-1.12"}}
  - filesystem: root
    path: /etc/coreos/docker-1.12
    mode: 0644
    contents:
      inline: yes
  {{end}}

  - filesystem: root
    path: /home/core/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /root/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /etc/systemd/timesyncd.conf
    mode: 0644
    contents:
      inline: |
        [Time]
        NTP=169.254.169.123

  - filesystem: root
    path: /etc/ssh/sshd_config
    mode: 0400
    contents:
      inline: |
        Subsystem sftp internal-sftp
        ClientAliveInterval 180
        AuthenticationMethods publickey
        UseDNS no
        UsePAM yes
        PrintLastLog no # handled by PAM
        PrintMotd no    # handled by PAM

  - filesystem: root
    path: /opt/bin/dns-conntrack-iptables
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables -I PREROUTING 1 -t raw -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I PREROUTING 1 -t raw -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK

        /usr/sbin/iptables -I INPUT 1 -t filter -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I INPUT 1 -t filter -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT

  - filesystem: root
    path: /opt/bin/drain-node
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          --net=host \
          docker://registry.opensource.zalan.do/teapot/kubectl:v1.13.7 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          label node "$(hostname)" \
          lifecycle-status=draining \
          --overwrite

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --net=host \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          docker://registry.opensource.zalan.do/teapot/kubectl:v1.13.7 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          drain "$(hostname)" \
          --ignore-daemonsets \
          --delete-local-data \
          --force
