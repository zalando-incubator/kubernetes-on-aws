# container linux config
passwd:
  users:
  - name: core
    ssh_authorized_keys:
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAt8djQXfn5U7H85oDzuZRfRONF+jVqn3Mp9t3tnBrJdKyTccfDovq1sekzEdOFdmj74yfS8bzaIO9pDUczy6j5k2MtDCoRnzAO6KZc46jMJ2GjhLArUuHLjmAw2r9LotZ30LEIEvJdmUI7mDlPMczv381PghyM8+DsYv62UrfjDiOsZ4EVkYnztQlO3ntNE16RFNj4fbfErQz67kmw0lB8C6bAf0RuRmvXzB7xRMplmknQnLusoURmySKdZM0GUe0VY6fmqOsgzHVLoEs1m82V8QK1ac/1DSHA91v50MbpCjTVLaRhjR8nmhWBedlhb6j5ClZdAQ8iwyyWC1MFQuvKw== henning@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICZh6WTi6o6ofWqJBmJjzYZdM/La5B/aZQLfpuho6rx7 mikkel.larsen@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIISHc8p8+ycB9H6SI/i3Uu/OI5G3vYNsZTn0DffPvfOA martin.linkhorst@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFVmr+Y3hBvnz2xgSWoaxaPxnExcEXOghUozupU9bKNR nick@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCq3oEP8qhMvGtlR1bgVc9tFOVJ5B5RMKtm6UQ/zXQUpm8DQ04SxdM2U7TfuLien2HSfpHAlYe0eLJZUfIqCXUeZ37v0ozj2RglireEcJm0t9XJ7kTS4kqVxrL6iuN6qQVGHs0vxoo/o9+SP0YkuuJoXwJvVI4yKVbnbfA5hKaAffAYPmfgqOZ7+3AMwmaj/D3tI0xVEA48ptGkj5nnOl0pXlfLRNvbnXOCa/dTKUgkma1F0lXoTipkRspsMEiAnwfJ1dwnzgNzllt//Ao/H+yOVR8fWJ7d+nowszIk6zwUR7c6walxKKf5Oy5bQBU49MZ6xLP1oma9F2+llmV7qpqx rodrigo.reis@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqAikaHsZWMuJvKZphZPZG0fnKMvVCRfBAbIS6e0Y+YqM0PfsWgB5e4f5TrbisQHdKopbfZVwYIaV/NegEuinrYPKC7t2ese/HjxgjHR95zHOcDP19Cbo+xeyH8zbRd9K3iRSyCUSMNRw5NL6zN8JOSl12m8QWQA4hTjFTmt870fIT4RLxu9qGlbQipUm57E/SotsNC41MQ/PsLQzOAviKrkS1rei2vzRHzAcjz1Z7GT5oH+dFVUC66kKa0XWDvq+VtkRVoLvS2chrIPCgESeeZAyOKyiOoyJxFFFiMVK48MWDBBIYTIsHE0qs/RwBi9+8lQGiHK5Rpk2djcloO0c7 sandor.szuecs@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINkh52Py+FvH9CRLDQg0gzvjEIrzwA45yMTXTsl2BVxV alexey.ermakov@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDakbukcXXmMyyxw5h3J8h41FK8b8/qjiX/u4BP+qQpQ3x/1VkppIpzHwLcjVThNspwOKKU6QwcaLKpJ+g3mYWdilsl7eFT5hAiIXq4OLe9XynXp1ZEduqKk7tgvjClo7OzuSdEfKtWkD35hzXWigVfJFZhSN9bLdiEzuIEHf0jEo2mUXADgAzSwbaTcVUreH/IR6SAYBVh4XgKoQjOmWrizJguYNk+DgBW6/R+5JQi8YYPSuevbcEKXXm1ZfOudNv2ebOL1rA4I+VDe2xW+JVwKIvAN3odt0zOH2NFr1kFU1GTVyD2ZYtyTNNae9a+mQ2Ltw5pzKjA4zjFctQrrI+yPEIcf64mccumIoRMjWozPB801VbZqUVyG0XhzHW7RNmTmmLsNa/STtYI9Xaqfrz2n3PgpYJ/bB42l4Ez9Oblh9rZU0aisr6risLh5XXgCa9NezMjTczMHJE0jVINgeIMsCleNF0HrkF26Uo1MJO1pOMZOGZzXSksB79TmTwjRdvQT/uvSAElBrthmRgj82pDsRdNoipp6VMx54m2KbvRyq79PVZP69NGWdAhJBw1SV0447kT1tuoqDTUskS4es1GoSu5sqtW4eF4Q6oju8+01l3ygLKmPPTbFVRcs6bBvHjnB3RYwgai5z0U16rsh7LKUwhYsRiar3lmrhVvoc95sw== muhammad.muaaz.saleem@zalando.de'
    - 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKIX5twzJYryUz6bw/0SLJRBg5RNfxUZiXOZ/1E75c2r arjun.naik@zalando.de'
    - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC6QTMJEdUHX/50ZzwelLGouD9mXpeHyTz0FW/vK1NPQHE0OM193bCXy0A+vjf7GaMUq8KOB8bdm96w3ffGM7ZKvyMOh4UHsxojabVZyKCaDafpQhzVSn1CRVy7vyJKoWkE5usuEbD2dKLxY5Svpw78+DHSdjNGTyFXPf/YGqNP2xrjTP96sx8yWg02w49kD3XrBQaFg5YicCHAAAwQ0IObx7dlT+6B9QrAuVY1nhIq1yqmKvLZa6h6MkoQPlp3Y3ksBhs0C+RNpB51EbBw2VMZJ+rbk/pOKDhlOMSDrLJb6TpAo6P7j1lvK0NrKpTrsifNPaJ5GtILa1hjVkMXpxJNDjc6+ApOzIuKhaACaufV/C7yKd1bGYFClkoDqxYck8mgQnBA+MAYWgLr4Bxu/R0Q1JwPiVDd6QGJcMSiBayIR5GT47X8SeV2yrNLBzbc7J3hctYNQxiNc+H2pxLr9P+yFCB6Xql0poFaG8vppAETPxeF1CaY86WsknPA5SqnDipr1O6OLJWcQMWw9iHj9mDMwyNpXJK3T5k/aoE+URZb2GxnTRzzcC3H+yi8ol3NsbyylHTSGwmbfbupzSenh2sSq7aZipDX6ELm0RbuogIbalOAtOUROGiG0HNcG8GbJLky6qMBSyh59VnaCDWPfFde1gsg+SH8Od3y2TOA+1rtbw== jannis.rake-revelant@zalando.de'
systemd:
  units:
  # disable automatic updates
  - name: update-engine.service
    mask: true
  - name: locksmithd.service
    mask: true

  # disable socket activation for sshd
  - name: sshd.service
    enable: true
  - name: sshd.socket
    mask: true

  - name: set-hostname.service
    enable: true
    contents: |
      [Unit]
      Wants=network.target
      Before=docker.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/hostnamectl set-hostname $(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-hostname)"

      [Install]
      WantedBy=multi-user.target

  - name: etcd-member.service
    enable: true
    contents: |
      [Unit]
      Wants=network.target

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5s
      StartLimitIntervalSec=0
      ExecStartPre=/usr/bin/mkdir --parents /var/lib/coreos
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/lib/coreos/etcd-member-wrapper.uuid
      ExecStart=/usr/bin/rkt run --uuid-file-save=/var/lib/coreos/etcd-member-wrapper.uuid --port=2379-tcp:2379 --mount volume=dns,target=/etc/resolv.conf --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true --insecure-options=image docker://registry.opensource.zalan.do/teapot/etcd-proxy:master-2 -- {{ .Cluster.ConfigItems.etcd_endpoints }}
      ExecStop=-/usr/bin/rkt stop --uuid-file=/var/lib/coreos/etcd-member-wrapper.uuid

      [Install]
      WantedBy=multi-user.target

  - name: docker.service
    dropins:
    - name: 40-flannel.conf
      contents: |
        [Service]
        EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
    - name: 60-dockeropts.conf
      contents: |
        [Service]
        Environment="DOCKER_OPTS=--log-opt=max-file=2 --log-opt=max-size=50m"
        Environment=DOCKER_SELINUX=

  - name: meta-data-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/meta-data-iptables.sh

      [Install]
      WantedBy=multi-user.target

  - name: dns-conntrack-iptables.service
    enable: true
    contents: |
      [Unit]
      After=private-ipv4.service

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=1
      ExecStart=/opt/bin/dns-conntrack-iptables

      [Install]
      WantedBy=multi-user.target

  - name: dockercfg.service
    enable: true
    contents: |
      [Unit]
      After=network.target

      [Service]
      Type=simple
      Restart=on-failure
      RestartSec=5
      ExecStartPre=/usr/bin/mkdir -p /root/.docker
      ExecStart=/opt/bin/dockercfg.sh

      [Install]
      WantedBy=multi-user.target

  - name: timesyncd-enable-network-time.service
    enable: true
    contents: |
      [Service]
      Type=oneshot
      ExecStart=/usr/bin/timedatectl set-ntp true

      [Install]
      WantedBy=multi-user.target

  - name: gen-controller-manager-config.service
    enable: true
    contents: |
      [Service]
      Type=oneshot
      ExecStart=/opt/bin/gen-controller-manager-config.sh

      [Install]
      WantedBy=multi-user.target

  - name: private-ipv4.service
    enable: true
    contents: |
      [Unit]
      After=network.target
      Description=Set PRIVATE_EC2_IPV4 env

      [Service]
      ExecStart=/usr/bin/bash -euo pipefail -c "/usr/bin/systemctl set-environment PRIVATE_EC2_IPV4=$(/usr/bin/curl --silent --fail http://169.254.169.254/latest/meta-data/local-ipv4)"
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

  - name: kubelet.service
    enable: true
    contents: |
      [Unit]
      After=docker.service dockercfg.service meta-data-iptables.service private-ipv4.service

      [Service]
      Environment=KUBELET_IMAGE_TAG=v1.12.5_custom.master-1
      Environment=KUBELET_IMAGE_URL=docker://registry.opensource.zalan.do/teapot/hyperkube
      Environment="RKT_RUN_ARGS=--insecure-options=image \
      --uuid-file-save=/var/run/kubelet-pod.uuid \
      --volume dns,kind=host,source=/etc/resolv.conf \
      --mount volume=dns,target=/etc/resolv.conf \
      --volume var-log,kind=host,source=/var/log \
      --mount volume=var-log,target=/var/log \
      --volume var-lib-cni,kind=host,source=/var/lib/cni \
      --mount volume=var-lib-cni,target=/var/lib/cni \
      --volume dockercfg,kind=host,source=/root/.docker/config.json \
      --mount volume=dockercfg,target=/root/.docker/config.json \
      --set-env=HOME=/root"
      ExecStartPre=/usr/bin/mkdir -p /var/log/containers
      ExecStartPre=/bin/mkdir -p /var/lib/cni
      ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
      ExecStart=/usr/lib/coreos/kubelet-wrapper \
      --cni-conf-dir=/etc/kubernetes/cni/net.d \
      --network-plugin=cni \
      --container-runtime=docker \
      --register-with-taints=node-role.kubernetes.io/master=:NoSchedule{{if index .NodePool.ConfigItems "taints"}},{{.NodePool.ConfigItems.taints}}{{end}} \
      --node-labels=node-role.kubernetes.io/master,kubernetes.io/role=master,master=true \
      --node-labels=cluster-lifecycle-controller.zalan.do/replacement-strategy=ensure-minimum-healthy-nodes \
      --node-labels=kubernetes.io/node-pool={{ .NodePool.Name }}{{if index .NodePool.ConfigItems "labels"}},{{.NodePool.ConfigItems.labels}}{{end}} \
      --node-labels=cluster-dns={{ .Cluster.ConfigItems.cluster_dns }} \
      --node-labels={{ .Values.node_labels }} \
      --pod-manifest-path=/etc/kubernetes/manifests \
      --cluster-dns=${PRIVATE_EC2_IPV4},10.3.0.11 \
      --cluster-domain=cluster.local \
      --kubeconfig=/etc/kubernetes/kubeconfig \
      --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
      --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem \
      --cloud-provider=aws \
      --feature-gates=TaintBasedEvictions=true,TaintNodesByCondition=false,ScheduleDaemonSetPods=false \
      --pod-infra-container-image=registry.opensource.zalan.do/teapot/pause-amd64:3.1 \
{{- if not (index .Cluster.ConfigItems "enable_cfs_quota") }}
      --cpu-cfs-quota=false \
{{- end }}
      --system-reserved=cpu=100m,memory=164Mi \
      --kube-reserved=cpu=100m,memory=282Mi
      ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  - name: kube-node-drainer.service
    enable: true
    contents: |
      [Unit]
      Description=drain this k8s node to make running pods time to gracefully shut down before stopping kubelet
      After=docker.service kubelet.service etcd-member.service

      [Service]
      Type=oneshot
      RemainAfterExit=true
      ExecStart=/bin/true
      TimeoutStopSec=120s
      ExecStop=/opt/bin/drain-node

      [Install]
      WantedBy=multi-user.target

storage:
  files:
  - filesystem: root
    path: /etc/kubernetes/kubeconfig
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: http://127.0.0.1:8080
        users:
        - name: kubelet
        contexts:
        - context:
            cluster: local
            user: kubelet

  - filesystem: root
    path: /etc/kubernetes/config/authn.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: authz-webhook
            cluster:
              server: http://127.0.0.1:8081/authentication
        users:
          - name: authz-webhook-client
            user:
              token: notused
        current-context: authz-webhook
        contexts:
        - context:
            cluster: authz-webhook
            user: authz-webhook-client
          name: authz-webhook

  - filesystem: root
    path: /etc/kubernetes/config/authz.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: authz-webhook
            cluster:
              server: http://127.0.0.1:8081/authorization
        users:
          - name: authz-webhook-client
        current-context: authz-webhook
        contexts:
        - context:
            cluster: authz-webhook
            user: authz-webhook-client
          name: authz-webhook

  - filesystem: root
    path: /etc/kubernetes/cni/docker_opts_cni.env
    mode: 0644
    contents:
      inline: |
        DOCKER_OPT_BIP=""
        DOCKER_OPT_IPMASQ=""

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-apiserver
          namespace: kube-system
          labels:
            application: kube-apiserver
            version: v1.12.5
          annotations:
            kubernetes-log-watcher/scalyr-parser: |
              [{"container": "webhook", "parser": "json-structured-log"}]
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          hostNetwork: true
          containers:
          - name: kube-apiserver
            image: registry.opensource.zalan.do/teapot/hyperkube:v1.12.5
            command:
            - /hyperkube
            - apiserver
            - --apiserver-count={{ .Values.apiserver_count }}
            - --bind-address=0.0.0.0
            - --insecure-bind-address=0.0.0.0
            - --etcd-servers=http://127.0.0.1:2379
            - --etcd-prefix={{ .Cluster.ConfigItems.apiserver_etcd_prefix }}
            - --storage-backend=etcd3
            - --storage-media-type=application/vnd.kubernetes.protobuf
            - --allow-privileged=true
            - --service-cluster-ip-range=10.3.0.0/16
            - --secure-port=443
            - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodSecurityPolicy,ImagePolicyWebhook,Priority,ExtendedResourceToleration
            - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
            - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1=true,policy/v1beta1/podsecuritypolicy=true,imagepolicy.k8s.io/v1alpha1=true,authorization.k8s.io/v1beta1=true,scheduling.k8s.io/v1alpha1=true,admissionregistration.k8s.io/v1beta1=true
            - --authentication-token-webhook-config-file=/etc/kubernetes/config/authn.yaml
            - --authentication-token-webhook-cache-ttl=10s
            - --cloud-provider=aws
            - --authorization-mode=Webhook,RBAC
            - --authorization-webhook-config-file=/etc/kubernetes/config/authz.yaml
            - --admission-control-config-file=/etc/kubernetes/config/image-policy-webhook.yaml
            - --feature-gates=TaintBasedEvictions=true,TaintNodesByCondition=false,ScheduleDaemonSetPods=false,TTLAfterFinished=true
            - --anonymous-auth=false
            {{ if or (eq .Cluster.Environment "production") (index .Cluster.ConfigItems "audittrail_url") }}
            - --audit-webhook-config-file=/etc/kubernetes/config/audit.yaml
            - --audit-webhook-mode=batch
            - --audit-policy-file=/etc/kubernetes/config/audit-policy.yaml
            {{ end }}
            # enable aggregated apiservers
            - --client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --requestheader-allowed-names=aggregator
            - --requestheader-extra-headers-prefix=X-Remote-Extra-
            - --requestheader-group-headers=X-Remote-Group
            - --requestheader-username-headers=X-Remote-User
            - --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem
            - --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                port: 8080
                path: /healthz
              initialDelaySeconds: 120
              timeoutSeconds: 15
            ports:
            - containerPort: 443
              hostPort: 443
              name: https
            - containerPort: 8080
              hostPort: 8080
              name: local
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /etc/kubernetes/config
              name: kubernetes-configs
              readOnly: true
            resources:
              requests:
                cpu: 100m
                memory: 200Mi
          - image: registry.opensource.zalan.do/teapot/admission-controller:master-9
            name: admission-controller
            readinessProbe:
              httpGet:
                scheme: HTTPS
                path: /healthz
                port: 8085
              initialDelaySeconds: 5
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 50m
                memory: 50Mi
            args:
              - --address=:8085
              - --tls-cert-file=/etc/kubernetes/ssl/admission-controller.pem
              - --tls-key-file=/etc/kubernetes/ssl/admission-controller-key.pem
              - --pod-default-cpu-request={{ .Cluster.ConfigItems.teapot_admission_controller_default_cpu_request }}
              - --pod-default-memory-request={{ .Cluster.ConfigItems.teapot_admission_controller_default_memory_request }}
              - --pod-ignore-namespaces={{ .Cluster.ConfigItems.teapot_admission_controller_ignore_namespaces }}
{{- if eq .Cluster.ConfigItems.teapot_admission_controller_process_resources "true" }}
              - --pod-process-resources
{{- end }}
            ports:
              - containerPort: 8085
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
          - image: registry.opensource.zalan.do/teapot/k8s-authnz-webhook:v0.5.0
            name: webhook
            ports:
            - containerPort: 8081
            livenessProbe:
              httpGet:
                path: /healthcheck
                port: 8081
              initialDelaySeconds: 30
              timeoutSeconds: 5
            readinessProbe:
              httpGet:
                path: /healthcheck
                port: 8081
              initialDelaySeconds: 5
              timeoutSeconds: 5
            resources:
              requests:
                cpu: 50m
                memory: 50Mi
            args:
              - --tokens-file=/etc/kubernetes/config/tokenfile.csv
            env:
              - name: TEAMS_API_URL
                value: https://teams.auth.zalando.com
              - name: CLUSTER_ID
                value: {{if index .Cluster.ConfigItems "webhook_id"}}{{ .Cluster.ConfigItems.webhook_id }}{{else}}{{ .Cluster.ID }}{{end}}
              - name: TOKEN_INTROSPECTION_URL
                value: http://127.0.0.1:9021/oauth2/introspect
              - name: USER_GROUPS
                value: credentials-provider=Administrator,credprov-kube-ops-view-read-only-token=ReadOnly,credprov-cluster-lifecycle-manager-cluster-rw-token=Administrator,stups_cluster-lifecycle-manager=Administrator,credprov-cluster-lifecycle-manager-test-cluster-rw-token=Administrator,credprov-cdp-controller-cluster-token=PowerUser,credprov-api-discovery-k8s-cluster-read-only-token=ReadOnly,stups_zmon-zmon=ReadOnly,stups_kube-resource-report=ReadOnly,kubelet=system:masters
              - name: BUSINESS_PARTNER_IDS
                value: {{ .Cluster.ConfigItems.apiserver_business_partner_ids }}
            volumeMounts:
            - mountPath: /etc/kubernetes/config
              name: kubernetes-configs
              readOnly: true
          - image: registry.opensource.zalan.do/foundation/platform-iam-tokeninfo:2fca26c
            name: tokeninfo
            ports:
              - containerPort: 9021
            livenessProbe:
              httpGet:
                path: /health
                port: 9021
              periodSeconds: 3
              failureThreshold: 2
            readinessProbe:
              httpGet:
                path: /health
                port: 9021
              periodSeconds: 3
              failureThreshold: 2
            resources:
              requests:
                cpu: 100m
                memory: 20Mi
            env:
              - name: OPENID_PROVIDER_CONFIGURATION_URL
                value: https://identity.zalando.com/.well-known/openid-configuration
              - name: ENABLE_INTROSPECTION
                value: "true"
          - image: registry.opensource.zalan.do/teapot/image-policy-webhook:v0.4.1
            name: image-policy-webhook
            args:
            - --policy={{ .Cluster.ConfigItems.image_policy }}
            - --failure-policy=fail
            ports:
            - containerPort: 8083
          - name: nginx
            image: registry.opensource.zalan.do/teapot/nginx:1.12.1
            ports:
            - containerPort: 8082
            resources:
              requests:
                cpu: 50m
                memory: 50Mi
            volumeMounts:
            - name: config-volume
              mountPath: /etc/nginx
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /etc/kubernetes/config
            name: kubernetes-configs
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /etc/kubernetes/nginx
            name: config-volume

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-controller-manager
          namespace: kube-system
          labels:
            application: kube-controller-manager
            version: v1.12.5
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          containers:
          - name: kube-controller-manager
            image: registry.opensource.zalan.do/teapot/hyperkube:v1.12.5
            command:
            - /hyperkube
            - controller-manager
            - --kubeconfig=/etc/kubernetes/controller-kubeconfig
            - --leader-elect=true
            - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --root-ca-file=/etc/kubernetes/ssl/ca.pem
            - --cloud-provider=aws
            - --cloud-config=/etc/kubernetes/cloud-config.ini
            - --feature-gates=TaintBasedEvictions=true,TaintNodesByCondition=false,ScheduleDaemonSetPods=false,TTLAfterFinished=true
            - --horizontal-pod-autoscaler-use-rest-clients=true
            - --use-service-account-credentials=true
            - --allocate-node-cidrs=true
            - --cluster-cidr=10.2.0.0/16
            - --horizontal-pod-autoscaler-use-rest-clients=true
            - --horizontal-pod-autoscaler-downscale-delay={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_downscale_delay }}
            - --horizontal-pod-autoscaler-sync-period={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_sync_period }}
            - --horizontal-pod-autoscaler-tolerance={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_tolerance }}
            - --horizontal-pod-autoscaler-upscale-delay={{ .Cluster.ConfigItems.horizontal_pod_autoscaler_upscale_delay }}
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 15
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /etc/kubernetes
              name: kubeconfig
              readOnly: true
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /etc/kubernetes
            name: kubeconfig
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host

  - filesystem: root
    path: /etc/kubernetes/manifests/kube-scheduler.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-scheduler
          namespace: kube-system
          labels:
            application: kube-scheduler
            version: v1.12.5
        spec:
          priorityClassName: system-node-critical
          tolerations:
          - key: node-role.kubernetes.io/master
            effect: NoSchedule
          hostNetwork: true
          containers:
          - name: kube-scheduler
            image: registry.opensource.zalan.do/teapot/hyperkube:v1.12.5
            command:
            - /hyperkube
            - scheduler
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            - --feature-gates=TaintBasedEvictions=true,TaintNodesByCondition=false,ScheduleDaemonSetPods=false
            env:
            - name: KUBE_MAX_PD_VOLS
              value: "26"
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 15

  - filesystem: root
    path: /etc/kubernetes/nginx/nginx.conf
    mode: 0644
    contents:
      inline: |
        user nginx;
        worker_processes 1;
        error_log /dev/stdout info;
        events {
          worker_connections 1024;
        }
        http {
          default_type application/octet-stream;
          server_tokens off;
          access_log off;
          server {
            listen 8082;
            server_name _;
            location / {
              if ($request_method != GET) {
                return 403;
              }
              proxy_pass_request_body off;
              proxy_pass http://127.0.0.1:8080;
            }
            location ~ /secrets(/|$) {
              return 403 "unauthorized";
            }
          }
        }

  - filesystem: root
    path: /opt/bin/gen-controller-manager-config.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        token="$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')"
        cat << EOF > /etc/kubernetes/controller-kubeconfig
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: https://127.0.0.1
            certificate-authority: /etc/kubernetes/ssl/ca.pem
        contexts:
        - name: local
          context:
            cluster: local
            user: kube-controller-manager
        current-context: local
        users:
        - name: kube-controller-manager
          user:
            token: ${token}
        EOF

        echo "${token},system:kube-controller-manager,system:kube-controller-manager" >> /etc/kubernetes/config/tokenfile.csv

  - filesystem: root
    path: /etc/kubernetes/config/tokenfile.csv
    mode: 0644
    contents:
      inline: |
        {{ .Cluster.ConfigItems.worker_shared_secret }},kubelet,kubelet

  - filesystem: root
    path: /etc/kubernetes/config/image-policy-webhook.yaml
    mode: 0644
    contents:
      inline: |
        imagePolicy:
          kubeConfigFile: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
          allowTTL: 2
          denyTTL: 2
          retryBackoff: 500
          defaultAllow: false

  - filesystem: root
    path: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
    mode: 0644
    contents:
      inline: |
        clusters:
          - name: image-policy-webhook
            cluster:
              server: http://127.0.0.1:8083/admit
        users:
          - name: image-policy-webhook
        current-context: image-policy-webhook
        contexts:
        - context:
            cluster: image-policy-webhook
            user: image-policy-webhook
          name: image-policy-webhook

  - filesystem: root
    path: /etc/kubernetes/config/audit.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: audit
          cluster:
            server: http://127.0.0.1:8889/audit
        contexts:
        - name: audit
          context:
            cluster: audit
            user: audit
        current-context: audit

  - filesystem: root
    path: /etc/kubernetes/config/audit-policy.yaml
    mode: 0644
    contents:
      inline: |
        apiVersion: audit.k8s.io/v1beta1
        kind: Policy
        rules:
          # The following requests were manually identified as high-volume and low-risk,
          # so drop them.
          - level: None
            users: ["system:kube-proxy"]
            verbs: ["watch"]
            resources:
              - group: "" # core
                resources: ["endpoints", "services", "services/status"]
          # don't audit any kubelet events
          - level: None
            users: ["kubelet"] # legacy kubelet identity
          # don't audit events from the system:unsecured user. This is the user
          # used when connecting to the apiserver over localhost, and will
          # usuaully be done by the local kubelet.
          - level: None
            users: ["system:unsecured"]
          # don't audit events from the system:apiserver user
          - level: None
            users: ["system:apiserver"]
          # don't audit any kube-controller-manager events
          - level: None
            users: ["system:kube-controller-manager"]
          # Don't audit events from system users in kube-system
          - level: None
            userGroups: ["system:serviceaccounts:kube-system"]
          # Don't audit events from high traffic infrastructure components
          - level: None
            users: ["credentials-provider"]
          - level: None
            userGroups: ["system:nodes"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["nodes", "nodes/status"]
          - level: None
            users:
              - system:kube-controller-manager
              - system:kube-scheduler
              - system:serviceaccount:kube-system:endpoint-controller
            verbs: ["get", "update"]
            namespaces: ["kube-system"]
            resources:
              - group: "" # core
                resources: ["endpoints"]
          - level: None
            users: ["system:apiserver"]
            verbs: ["get"]
            resources:
              - group: "" # core
                resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
          # Don't log HPA fetching metrics.
          - level: None
            users:
              - system:kube-controller-manager
            verbs: ["get", "list"]
            resources:
              - group: "metrics.k8s.io"
          # Don't log these read-only URLs.
          - level: None
            nonResourceURLs:
              - /healthz*
              - /version
              - /swagger*
          # Don't log events requests.
          - level: None
            resources:
              - group: "" # core
                resources: ["events"]
          # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
          # so only log at the Metadata level.
          - level: Metadata
            resources:
              - group: "" # core
                resources: ["secrets", "configmaps"]
              - group: authentication.k8s.io
                resources: ["tokenreviews"]
            omitStages:
              - "RequestReceived"
          # Don't audit read-only events.
          - level: None
            verbs: ["watch", "list", "get"]
          # Default level for all other requests.
          - level: Request
            omitStages:
              - "RequestReceived"

  - filesystem: root
    path: /etc/kubernetes/ssl/ca.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.ca_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/apiserver.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.apiserver_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/apiserver-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.apiserver_key_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/worker.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_cert_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/worker-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.worker_key_decompressed }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/proxy-client.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.proxy_client_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/proxy-client-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.proxy_client_key }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/admission-controller.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.admission_controller_cert }}"

  - filesystem: root
    path: /etc/kubernetes/ssl/admission-controller-key.pem
    mode: 0664
    contents:
      remote:
        url: "data:text/plain;base64,{{ .Cluster.ConfigItems.admission_controller_key }}"

  - filesystem: root
    path: /etc/kubernetes/cloud-config.ini
    mode: 0644
    contents:
      inline: |
        [global]
        DisableSecurityGroupIngress = true

  - filesystem: root
    path: /opt/bin/dockercfg.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        iid="instance-identity-document:$(curl --silent --fail http://169.254.169.254/latest/dynamic/instance-identity/pkcs7)"
        iid="$(echo -n "$iid" | base64 -w 0)"
        cat << EOF > /root/.docker/config.json
        {
          "auths": {
            "https://pierone.stups.zalan.do": {
              "auth": "${iid}"
            }
          }
        }
        EOF

  - filesystem: root
    path: /opt/bin/meta-data-iptables.sh
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables \
          --append PREROUTING \
          --protocol tcp \
          --destination 169.254.169.254 \
          --dport 80 \
          --in-interface cni0 \
          --match tcp \
          --jump DNAT \
          --table nat \
          --to-destination ${PRIVATE_EC2_IPV4}:8181

  {{if index .Cluster.ConfigItems "docker-1.12"}}
  - filesystem: root
    path: /etc/coreos/docker-1.12
    mode: 0644
    contents:
      inline: yes
  {{end}}

  - filesystem: root
    path: /home/core/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /root/.toolboxrc
    mode: 0644
    contents:
      inline: |
        TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
        TOOLBOX_DOCKER_TAG=v0.0.6

  - filesystem: root
    path: /etc/systemd/timesyncd.conf
    mode: 0644
    contents:
      inline: |
        [Time]
        NTP=169.254.169.123

  - filesystem: root
    path: /opt/bin/dns-conntrack-iptables
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/sbin/iptables -I PREROUTING 1 -t raw -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I PREROUTING 1 -t raw -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK
        /usr/sbin/iptables -I OUTPUT 1 -t raw -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j NOTRACK

        /usr/sbin/iptables -I INPUT 1 -t filter -p udp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I INPUT 1 -t filter -p tcp -d "${PRIVATE_EC2_IPV4}" --dport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p udp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT
        /usr/sbin/iptables -I OUTPUT 1 -t filter -p tcp -s "${PRIVATE_EC2_IPV4}" --sport 53 -j ACCEPT

  - filesystem: root
    path: /opt/bin/drain-node
    mode: 0755
    contents:
      inline: |
        #!/bin/bash
        set -euo pipefail

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          --net=host \
          docker://registry.opensource.zalan.do/teapot/hyperkube:v1.12.5 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          label node "$(hostname)" \
          lifecycle-status=draining \
          --overwrite

        /usr/bin/rkt run --insecure-options=image \
          --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
          --mount=volume=kube,target=/etc/kubernetes \
          --net=host \
          --volume dns,kind=host,source=/run/systemd/resolve/resolv.conf,readOnly=true \
          --mount volume=dns,target=/etc/resolv.conf \
          docker://registry.opensource.zalan.do/teapot/hyperkube:v1.12.5 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          drain "$(hostname)" \
          --ignore-daemonsets \
          --delete-local-data \
          --force
