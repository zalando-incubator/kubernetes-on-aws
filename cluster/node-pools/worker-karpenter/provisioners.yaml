{{ if eq .Cluster.ConfigItems.karpenter_pools_enabled "true"}}
---
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: "{{.NodePool.Name}}"
spec:
  kubeletConfiguration:
    maxPods: {{ nodeCIDRMaxPods (parseInt64 .Cluster.ConfigItems.node_cidr_mask_size) (parseInt64 .Cluster.ConfigItems.node_max_pods_extra_capacity) }}
    systemReserved:
      cpu: "{{ .Cluster.ConfigItems.kubelet_system_reserved_cpu }}"
      memory: "{{ .Cluster.ConfigItems.kubelet_system_reserved_memory }}"
    kubeReserved:
      cpu: "{{ .Cluster.ConfigItems.kubelet_kube_reserved_cpu }}"
      memory: "{{ .Cluster.ConfigItems.kubelet_kube_reserved_memory }}"
  {{- if index .NodePool.ConfigItems "scaling_priority"}}
  weight: {{.NodePool.ConfigItems.scaling_priority}}
  {{- end}}
  consolidation:
    {{- if index .NodePool.ConfigItems "consolidation" }}
    enabled: {{ .NodePool.ConfigItems.consolidation }}
    {{- else }}
    enabled: true
    {{- end }}
  requirements:
{{- if and (eq (len .NodePool.InstanceTypes) 1) (eq (index .NodePool.InstanceTypes 0) "default-for-karpenter") }}
    - key: "karpenter.k8s.aws/instance-family"
      operator: In
      values:
        - "c5"
        - "m5"
        - "r5"
        - "c5d"
        - "m5d"
        - "r5d"
        - "c5n"
        - "m5n"
        - "r5n"
        - "c6i"
        - "m6i"
        - "r6i"
        - "c6id"
        - "m6id"
        - "r6id"
        - "c6in"
        - "m6in"
        - "r6in"
    - key: "karpenter.k8s.aws/instance-size"
      operator: "NotIn"
      values:
        - "metal"
{{- else }}
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values:
      {{- range $type := .NodePool.InstanceTypes }}
      - "{{ $type }}"
      {{- end }}
{{- end }}
    - key: "karpenter.sh/capacity-type"
      operator: In
      values:
        - "spot"
        - "on-demand"
    - key: "kubernetes.io/arch"
      operator: In
      values: ["arm64", "amd64"]
    - key: "topology.kubernetes.io/zone"
      operator: In
      values:
{{- $azs := .Values.availability_zones }}
{{- if index .NodePool.ConfigItems "availability_zones"}}
        {{    $azs = split .NodePool.ConfigItems.availability_zones "," }}
{{- end}}
{{- range $az :=  $azs }}
      - "{{ $az }}"
{{- end }}
{{- $taints := .NodePool.Taints}}
{{- if $taints }}
  taints:
  {{- range $taints }}
    {{- $taint := . }}
      - key: {{$taint.Key}}
        effect: {{$taint.Effect}}
        {{- if $taint.Value }}
        value: {{$taint.Value }}
        {{- end }}
  {{- end}}
{{- end}}

  startupTaints:
  - key: zalando.org/node-not-ready
    effect: NoSchedule

  labels:
    # these labels are normally set by kubelet on start-up, but because
    # karpenter already creates the node object ahead of the instance start, we
    # need to set them here as well.
    lifecycle-status: ready
    node.kubernetes.io/node-pool: "{{ .NodePool.Name }}"
    node.kubernetes.io/role: worker
    node.kubernetes.io/profile: "{{ .NodePool.Profile }}"
    cluster-lifecycle-controller.zalan.do/replacement-strategy: none
{{- if index .NodePool.ConfigItems "labels"}}
  {{- range split .NodePool.ConfigItems.labels ","}}
    {{- $label := split . "="}}
    {{index $label 0}}: {{index $label 1}}
  {{- end}}
{{- end}}

  # ttlSecondsAfterEmpty: 30
  providerRef:
    name: {{ .NodePool.Name }}
---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: {{ .NodePool.Name }}
spec:
  amiFamily: Custom
  amiSelector:
    aws-ids: "{{ index .NodePool.ConfigItems (print "kuberuntu_image_v1_24_" .NodePool.ConfigItems.kuberuntu_distro_worker "_amd64")  }},{{ index .NodePool.ConfigItems (print "kuberuntu_image_v1_24_" .NodePool.ConfigItems.kuberuntu_distro_worker "_arm64") }}"
  metadataOptions:
    httpTokens: required
    httpPutResponseHopLimit: 2
  subnetSelector:
    kubernetes.io/role/karpenter: "enabled"
  securityGroupSelector:
    karpenter.sh/discovery: "{{ .Cluster.ID }}/WorkerNodeSecurityGroup"
  blockDeviceMappings:
    - deviceName: /dev/sda1
      ebs:
        deleteOnTermination: {{ .NodePool.ConfigItems.ebs_root_volume_delete_on_termination}}
        volumeSize: {{ .NodePool.ConfigItems.ebs_root_volume_size }}Gi
        volumeType: gp3
  networkInterfaces:
    - associatePublicIPAddress: true
      deviceIndex: 0
  userData: |
{{.UserData | indent 4}}
  tags:
    Name: "{{ .NodePool.Name }} ({{ .Cluster.ID }})"
    node.kubernetes.io/role: worker
    node.kubernetes.io/node-pool: "{{ .NodePool.Name }}"
    zalando.org/pod-max-pids: "{{ .NodePool.ConfigItems.pod_max_pids }}"
    'zalando.de/cluster-local-id/{{ .Cluster.LocalID }}': owned
{{- if and (eq .Cluster.ConfigItems.skipper_attach_only_to_skipper_node_pool "true") (eq (index .NodePool.ConfigItems "taints") "dedicated=skipper-ingress:NoSchedule") }}
    zalando.org/ingress-enabled: "true"
# only node pools without taints should be attached to Ingress Load balancer
{{- else if and (not (eq .Cluster.ConfigItems.skipper_attach_only_to_skipper_node_pool "true")) (or (not (index .NodePool.ConfigItems "taints")) (eq (index .NodePool.ConfigItems "taints") "dedicated=skipper-ingress:NoSchedule")) }}
    zalando.org/ingress-enabled: "true"
{{- end }}

{{- if $taints }}
  {{- range $taints }}
    {{- $taint := . }}
    {{- if eq $taint.Key "dedicated"}}
    dedicated: "{{ $taint.Value}}"
    {{- end}}
  {{- end}}
{{- end}}

{{- end }}
