{{ if eq .Cluster.Environment "production" }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: emergency-access-service
  namespace: kube-system
  labels:
    application: emergency-access-service
spec:
  replicas: 1
  selector:
    matchLabels:
      application: emergency-access-service
  template:
    metadata:
      labels:
        application: emergency-access-service
      annotations:
        kubernetes-log-watcher/scalyr-parser: |
          [{"container": "emergency-access-service", "parser": "json-structured-log"}]
        logging/destination: "{{.Cluster.ConfigItems.log_destination_both}}"
    spec:
      dnsConfig:
        options:
          - name: ndots
            value: "1"
      priorityClassName: "{{ .Cluster.ConfigItems.system_priority_class }}"
      serviceAccountName: emergency-access-service
      containers:
      - name: apiserver-proxy
        image: container-registry.zalando.net/teapot/etcd-proxy:master-12
        args:
        - "--listen-address"
        - "127.0.0.1:333"
        - "$(KUBERNETES_SERVICE_HOST):$(KUBERNETES_SERVICE_PORT)"
        resources:
          requests:
            cpu: 25m
            memory: 25Mi
          limits:
            cpu: 25m
            memory: 25Mi
      - name: emergency-access-service
        image: "container-registry.zalando.net/teapot/emergency-access-service:master-95"
        args:
        - --insecure-http
        - --community={{ .Cluster.Owner }}
        - --cluster-id={{ .Cluster.ID }}
        - --teams-api-url=https://teams.auth.zalando.com
        # TODO(mlarsen): Rename this flag to tokeninfo-url to reflect that it's
        # not the OAuth2 token URL.
        - --token-url=https://info.services.auth.zalando.com/oauth2/tokeninfo
        # enable audittrail reporting
        - --audittrail-url=https://audittrail.cloud.zalando.com
        - --environment={{ .Cluster.Environment }}
        - --s3-bucket-name=zalando-audittrail-{{accountID .Cluster.InfrastructureAccount}}-{{ .Cluster.LocalID }}
        env:
        - name: KUBERNETES_SERVICE_HOST
          value: "127.0.0.1"
        - name: KUBERNETES_SERVICE_PORT
          value: "333"
        - name: CONFIGMAP_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: OPSGENIE_API_KEY
          valueFrom:
            secretKeyRef:
              name: emergency-access-service-secrets
              key: opsgenie-api-key
        - name: AWS_REGION
          value: "{{ .Cluster.Region }}"
        - name: OPENTRACING_LIGHTSTEP_ACCESS_TOKEN
          valueFrom:
            secretKeyRef:
              name: "emergency-access-service-secrets"
              key: opentracing-lightstep-access-token
        - name: OPENTRACING_LIGHTSTEP_COLLECTOR_HOST
          value: "{{ .Cluster.ConfigItems.tracing_collector_host }}"
        - name: OPENTRACING_LIGHTSTEP_COLLECTOR_PORT
          value: "8444"
        - name: OPENTRACING_LIGHTSTEP_COMPONENT_NAME
          value: "emergency-access-service"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: platform-iam-credentials
          mountPath: /meta/credentials
          readOnly: true
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          timeoutSeconds: 1
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 50m
            memory: 100Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
      volumes:
      - name: platform-iam-credentials
        secret:
          secretName: "emergency-access-service"
{{ end }}
