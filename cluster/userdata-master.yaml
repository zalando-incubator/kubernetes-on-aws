#cloud-config
ssh_authorized_keys:
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC8yXjMX812gbdUsosLvIA/zPrRZI+iOKo8Igq7eiwRTeJKxIkRd2vxDGDfqzCnMhUPPgLW7YM0FWt1u8ZTlXyL4fv1KZuYMRF+27GuU2b/IKu4JYlHDDHrvKCr9tyKb+YVPrAfzi2xVRTGGZG6GRmAiDw8HSUFdejuDLIropAFQu9gAow9aiuQG6kmxZK8qdwIu03eZhuUHCRySCPbusjO+I2JEyVa7lM+P1zt2znDwXsdGjTzHE7NSu5z/VHzho3STWolBm5Vk8uLNYKhjs0eiw/FXV5t+c08Y3JA1LwjAmfOBS23ERSHMG8I3EVSnB9utrdLXejwFAV/jY+Dl2QX9o8brFePBe26MbSg9udD0yfrVz4HEG6NigK6sSx97Zs0lcaSwvjJsnp/J3B1IQMlNRLJGQ7WxM9H2rtoMmD7/iPdADevehEui8C9iaADk6j8AWtN0SJbccSvidrFXW7eH/YSYW394rk8Cl1xBk2RORv3OGVy/RNVt6/Pk/BzvqKf3RvKlbvbFZsWeBSZHDAKeer7001g8HmKV7c8fnDsxIU9Ro3aeWpsX1rQ/jzH1an2deXzVDX1Xbm80VmL5M53dr4w2ZiF5uEIODEUr2nssvl6fhdnaXmbO0vsIyOVawl8mkQ7CBsW06Q+kNs7iDvwVmoG/DD0k4i86La2TBpGvQ== andre.hartmann@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAt8djQXfn5U7H85oDzuZRfRONF+jVqn3Mp9t3tnBrJdKyTccfDovq1sekzEdOFdmj74yfS8bzaIO9pDUczy6j5k2MtDCoRnzAO6KZc46jMJ2GjhLArUuHLjmAw2r9LotZ30LEIEvJdmUI7mDlPMczv381PghyM8+DsYv62UrfjDiOsZ4EVkYnztQlO3ntNE16RFNj4fbfErQz67kmw0lB8C6bAf0RuRmvXzB7xRMplmknQnLusoURmySKdZM0GUe0VY6fmqOsgzHVLoEs1m82V8QK1ac/1DSHA91v50MbpCjTVLaRhjR8nmhWBedlhb6j5ClZdAQ8iwyyWC1MFQuvKw== henning@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIgO42Rr98DRo4N86Uk4cqHcnbA0iPhL7/DQXRNs+o2vfnfoTVNQiyfq8XzScb+pRJpiq2wW2OthMVRUJsiO4PECw7avYAi0M8cyCsu+ZUoIeMlu+TssWA00GZgdKcQKyCoyLlGbPu2z4GpM9tX+7ASMbMuk6fpm6n9af1YbTm2eqKKXpHDJO+aex3WCj1VyQpCgCD4zquGRy8JRSiQ+QGyGZHIzWWpo3Lc1xGqSQu6L3h4RMrwtZNOjMr/xxxnApOQjBLr70Q8MlYnAhXrIyLjNOaMabMKVxDEltrvN2rCWfN/DFyRi4c5GiDKk1/lwlfO9dRieUqXm9J670PWhwx jan.mussler@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDOjGGBqrWGdbh0yfkBRE04IbdiAP2TJ2RXnfnp3OV8Lw+jg7cUqeZA5l2vIhFd2ctKPd5OGE3K/A6xCK3hFV9V01oYbH8S3IvrhyA+VjBG0d/ZgTm6GCrlE4XeAt9/ourBSxrrE04lfO786dhLsGEOa5WvvSG3Z/6BhyC/1e5Bd37nnWB363fjAsbg1UgSPr99QZQ5l2mSxN4i2IpJjULBWpLvrJLLJzzl67aaVhDjdtggEU+pMsOoRpDuJ46cYMMDvBI9gyyal2G1aIkqu9iejt1bly53Th2ZAiXecXxEh4K3a5H/Czf70vpCzXQiG1OuZRD1PaSpqw4+zzcizZdX matthias@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCs4NhH7uwlnklYYlV1GP57XX9NHYgXNv0njfEVowR1jFSKiMdEUtPKz5lOAeDtKpck9HyjxVOTSutNOIBZkkgN/FgkcpNbc59nIO4ELUzipkJivuYNO1lDAjCQi1qwYo+uksiNfcsao9mn7nd9Rh1g8TXYv95TSvml9v/Dolmanm1qj6OkbDw1xIurnNsoCjdxCWwSRGNja4r+PQc8pi+1xpdUBEvn40CeBdU7b/hZnv/BM9FIKSsVlIwMR2i/Co2rxCKo9B3q4dmdQ/2C1QczINVpPQcNUMuiljJFMXvT7dgfNsnUBe8vFuoPgqohJ/m8AiKZZOyoRNiCuELnKLRKqxosDmJz47Y0YiYgZk9jpnmt+x1fwqhY2R85F7W4RybpX3AFL1jOqOT2lE+idkhyeFq/pPoAiPvUcpQSmL9AwlrG6cPklTJZW+dUzH/W6kNlgl/T6hnMn4Mu+IljG08Iyb/HBdmOX+uRq5wdF8lI9Zjzlwg+j7HMa3p1O7MNwpSJVjVXkhUXH0WrFrK2JkwgXokIWvu0o+lajYOmXRQeGCyVybg06WFCzOXnK2toVucFMuAGoM0NkthAnodZCk+xXLNrtHOYagpdJ/x/Q88vuDsZr7IG0NvgX/OCq1a5lpnIydCe+tABNNKcRaOuOLbDYVgUaeVzIXyJjRYu5ZN/JQ== mikkel.larsen@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAEAQCry4lt7DKc4yQpWIsvIIzWlBYBJifhX+1BZueb8jXpeVyiqL4U4S/XyR4QPRvIxB5JY1rc1y/Mo6+J6XpbhIfSJROMSaFWcsisRj2/lmvrU9RmhSefpQgB04qQcXejy6WqCyYYQSUwwbbObmq/mFZLt4iMI3J5oDyN5Fg5+8JXb4udDZQLJuH7qIkt+MZuy8v7HtSK03lEmDlAm14D9zndFEfSDfituUk/X/K5N7Li6dGYf4JLqIuX5yf2HoNyCCDrtUm9E/pUvBxrnPEtALBuMf8hd+hgNhJmiX2sZgSQHV9XhYLlNUshw+uZ7YVFOaYi3OQUn5pJVKFuxF5LftsXWl1xlFRy7SS2Vf5emMBXOmVTcDKFIghpojtsmsq53lvu2YSBZ6BJP2cqgD5HLw3xgqn6uzYaJCbCdiJZsGQ7Gly/fShR0ebOuvbumJpxegCE54pps3icwhvEhqbWhTv+z/zuvH1z5rqod1+GvrH6kjA80+cjv56zgLR77EHXnhwrTsDgb5KWm9Qktl/bbfnrYzT889/4gzE+3IO/RBHgM12aAHFCc1G9Kp/25iEK/2b3t6Qw1sikNo/0BRD4Y1CakTXtZOvp7nNOCZSAUY0PXsrKF82sDWIgccgeiNebxS1ESU0JQVDzzKZ/HmOP5fCRFC3nQnZtk/5/nj6iRiH22gcRl77Cv3k7MpX9MnpVE5Duk7VL0wnVQ8xJ7mNbUSzadsyJHbvBjfsWL2C+s6LC0ApAVBXekcymvkqP7nZwPj8XCrN0nWhBBGym6VUwDdLI9dedWd3XJrhgmbmc74xOhYHTTNVpiuHpYNb90G+0O/GHWaAY76fg4YvR+OwZVbPu8a36EhYMuzsNEshzwnAxh/1PHrGH7X2aHc+E37f5/hTL9QArqdiIhqt0iMCXMF89mlNlQ8oC6FgmaFlMub4fX7Sy6tkzl3W9pqQvAVlFxvCQ2zApYYsMwJDgwQRkxCX0ohzDYKvnFXLHGi3M+mKZQfDEe2KNOJaB8vBc3G0veTiH3fnveB/NtmsvYiBBQn+Z/Ftiezb8ApnE6GJofKBVA+ijCO4eKyYOuozik+oKJ4F2wLnDZM2rRJU/uxZhGrg50cPv62Hszhc3m3WGLMHJ9lntK1mDpZds6xbF4DccsE81Of11hZnTE8sookAKRt1FIL+kZd564JWaD9iM3SQuTXdCK/46Ms+Dmr5/MCMlGSNyRw52OZ1m1R5s+yMDBddGwMf5xh6s5Mh2G9bODETueEDPlYNiQCQw5Rx19ZT/W/CaaPswTYPYIgAQNXg8XnWzdKDCTqZ2z6LMwfyEAgaHIPoubLPVKk0iPWYnS2BpfrxdeG/DeywDVf2GEgnkdaVD martin.linkhorst@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDf3X7MZjYRZD2mI7dvW+c/8dUeDrdqAKZD/C9r+aGa6hWzbGeUZ7cub8to6X9Cl5p3MsTdtFV/OIHRIvxoIGYkz3CRHEBtUdUwOjF9lpBoB/yRMJyxlogmMCm9KGSUM4K+xgIX6qBHq/UeY2Yqumec5lhuLk+7wTmXYQM3+fvvHB8MY//UEadvjuDdotNGQ4jxkJjfoTQj6dspvsZCJ4kIIef9DLSJQ4oCV7sDCVDWPllb3ni9WJYD4vTguI82DI0moQV0WIPplH6rrK+ctVmPyix/IertpqWbiLgmdz3SAyGW45uws2ozGB1S+tZJF+RcNFbbAimoz6QHT+kgL1qmpxa7yba7y3m5pUHqGdhLb+X4Xe3oMZGk7cwBOECw8JUzxzqQKxpF1PfbH8wJ8AiKF7Xr/KJNk9Axsm1zV0DDWv3Z2oK7m8pNiq3mlhv6ovIpXsTq40uaasuLSfmLjSagQDT6ufBAUbaSAJfM0VFo4diIOCDnXH0SX1T8X3EPKbyDg0l/y0pE+FxXQvBK2rzgeynoK5NrvGl1xhJGetbMsl0+WtnIr/PbIQDTo9UQAzOWnHsTs72VqNbeJN4w8ksTqNhXiQO6zlhWqoPL/BeXvRc4N0R8iq9vIstjtuAZkaFsm5TH7Uzz3WTHzKbJ0/5+sefX4cucb/QjImvcVV1UEw==
 nick.juettner@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCuFg0sZtpORWFESaGFnDHYPDG2mYAEsg3pz2yvLn8L2U4b+clIqzQyMjjnze/syB979hVxAucb5A7YR8s7jPIcHUsTcLdGJUNATxiYSdReFj+6ki+6vk01zf9fTVrmAy39SYMi0hSicyMpQL7ur1osnqZ/OEq7+m/qJD6dQ5KpSMVr72kcdXqObEGxIKwPSJlyZ2df1UoMxuslLeGzovuNfngFMTpE7K6A1Nuysn0Vm4EIJ1/UC1nADbTsRexpwCD/7ZanS349RGhGA7SLeXKwSEejL6IkaMF6mWyy4+IWeKraSogw73oM9Quyzq72AwZos4WTb61DVWcRaRXj0D1/ raffaele.di.fazio@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCq3oEP8qhMvGtlR1bgVc9tFOVJ5B5RMKtm6UQ/zXQUpm8DQ04SxdM2U7TfuLien2HSfpHAlYe0eLJZUfIqCXUeZ37v0ozj2RglireEcJm0t9XJ7kTS4kqVxrL6iuN6qQVGHs0vxoo/o9+SP0YkuuJoXwJvVI4yKVbnbfA5hKaAffAYPmfgqOZ7+3AMwmaj/D3tI0xVEA48ptGkj5nnOl0pXlfLRNvbnXOCa/dTKUgkma1F0lXoTipkRspsMEiAnwfJ1dwnzgNzllt//Ao/H+yOVR8fWJ7d+nowszIk6zwUR7c6walxKKf5Oy5bQBU49MZ6xLP1oma9F2+llmV7qpqx rodrigo.reis@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCqAikaHsZWMuJvKZphZPZG0fnKMvVCRfBAbIS6e0Y+YqM0PfsWgB5e4f5TrbisQHdKopbfZVwYIaV/NegEuinrYPKC7t2ese/HjxgjHR95zHOcDP19Cbo+xeyH8zbRd9K3iRSyCUSMNRw5NL6zN8JOSl12m8QWQA4hTjFTmt870fIT4RLxu9qGlbQipUm57E/SotsNC41MQ/PsLQzOAviKrkS1rei2vzRHzAcjz1Z7GT5oH+dFVUC66kKa0XWDvq+VtkRVoLvS2chrIPCgESeeZAyOKyiOoyJxFFFiMVK48MWDBBIYTIsHE0qs/RwBi9+8lQGiHK5Rpk2djcloO0c7 sandor.szuecs@zalando.de'
  - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCa6Sj76tQ/nyCbkAd5zL/ZPhH+4lfuBT7fFXfamRnIiA3LVXp5/0R8vhvXOYL5Q6uclr5n8sJD1IcwpYTkQrV+u7+PKnbBJZjYDgsbcDr843jzDHhJOzWiKpB5fFcu4Ll5pYVyXq3CgsTuNKBUCnQJEtx7WLRBAFoHZuicp9IjfXfyEVclcKvlGfWBFnidaSCrv2MKKCfqYwif8bY1PSnDYaqsqCrptZkzvuFlcewtsp2t12W760LiZlAB8RRM9SE5DdTYqyeLB2NymI0JecFM+yW/p3bRp6NZt63w9N/IqjQw5aJwAJfEzrb2dVKlr9y7TXyKZPfElvzPEsujTZBOTZs4uRfTMdPsAJj8qB/hNvdIfaFLn9KNQvzhwOjk8zjYU5NhZMUu5T8VBby8tcg9DLFfIqLJxoD1E1UHkOXCIuA3jhF6zre2d6cUeLmatsuOmd2OYeAcy9s3GOOjn/zCpIZ9k5eDLaWKwBgNFnNVznWdOgpd5BaY5PE1wS6655/po7hEngE3KWaNcmHV7AuQteozWYNNbIZy78UZ2p9LDm7v78BP5UNy834kSOu8MUQN2hNJ9hZ4x0MI2F6nmDKxqxAfkH9LBQ3Mf6FfJsV0+GSrLnI0mvmBcZp9/LOwcBAjf9MyHN3IcFbTxiQATaj7rXnYoBi88g9l4lQObiBNXQ== yerken.tussupbekov@zalando.de'
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_endpoints: http://127.0.0.1:2379
  units:
    - name: etcd-member.service
      command: start
      enable: true
      drop-ins:
        - name: 40-etcd-gateway.conf
          content: |
            [Service]
            Environment="ETCD_IMAGE_TAG=v3.1.6"
            Environment="ETCD_OPTS=gateway start --listen-addr=127.0.0.1:2379 --endpoints={{ ETCD_ENDPOINTS }}"

    - name: docker.service
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
        - name: 60-dockeropts.conf
          content: |
            [Service]
            Environment="DOCKER_OPTS=--log-opt=max-file=2 --log-opt=max-size=50m"

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Service]
            ExecStartPre=/usr/bin/etcdctl \
            --endpoint=http://127.0.0.1:2379 set /coreos.com/network/config \
            '{ "Network": "10.2.0.0/16", "Backend": {"Type": "vxlan"}}'

    - name: kube2iam-iptables.service
      command: start
      runtime: true
      content: |
        [Service]
        Type=oneshot
        ExecStart=/usr/sbin/iptables \
          --append PREROUTING \
          --protocol tcp \
          --destination 169.254.169.254 \
          --dport 80 \
          --in-interface cni0 \
          --match tcp \
          --jump DNAT \
          --table nat \
          --to-destination $private_ipv4:8181

        [Install]
        WantedBy=multi-user.target

    - name: dockercfg.service
      command: start
      runtime: true
      content: |
        [Service]
        Type=oneshot
        ExecStartPre=/usr/bin/mkdir -p /root/.docker
        ExecStart=/opt/bin/dockercfg.sh

        [Install]
        WantedBy=multi-user.target

    - name: gen-controller-manager-config.service
      command: start
      runtime: true
      content: |
        [Service]
        Type=oneshot
        ExecStart=/opt/bin/gen-controller-manager-config.sh

        [Install]
        WantedBy=multi-user.target

    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Service]
        Environment=KUBELET_IMAGE_TAG=v1.7.4_coreos.0
        Environment=KUBELET_IMAGE_URL=docker://registry.opensource.zalan.do/teapot/hyperkube
        Environment="RKT_RUN_ARGS=--insecure-options=image \
        --uuid-file-save=/var/run/kubelet-pod.uuid \
        --volume dns,kind=host,source=/etc/resolv.conf \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log \
        --volume var-lib-cni,kind=host,source=/var/lib/cni \
        --mount volume=var-lib-cni,target=/var/lib/cni \
        --volume dockercfg,kind=host,source=/root/.docker/config.json \
        --mount volume=dockercfg,target=/root/.docker/config.json \
        --set-env=HOME=/root"
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=/bin/mkdir -p /var/lib/cni
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --network-plugin=cni \
        --container-runtime=docker \
        --rkt-path=/usr/bin/rkt \
        --register-schedulable=false \
        --allow-privileged \
        --node-labels=master=true \
        --node-labels={{NODE_LABELS}} \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster_dns=10.3.0.10 \
        --cluster_domain=cluster.local \
        --kubeconfig=/etc/kubernetes/kubeconfig \
        --require-kubeconfig \
        --tls-cert-file=/etc/kubernetes/ssl/worker.pem \
        --tls-private-key-file=/etc/kubernetes/ssl/worker-key.pem \
        --cloud-provider=aws \
        --low-diskspace-threshold-mb=1000 \
        --feature-gates=AllAlpha=true \
        --system-reserved=cpu=100m,memory=164Mi \
        --kube-reserved=cpu=100m,memory=282Mi
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

    - name: kube-node-drainer.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Requires=docker.service
        After=docker.service

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStart=/bin/true
        TimeoutStopSec=120s
        ExecStop=/bin/sh -c '/usr/bin/rkt run \
        --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
        --mount=volume=kube,target=/etc/kubernetes \
        --net=host \
        registry.opensource.zalan.do/teapot/hyperkube:v1.7.4_coreos.0 \
          --exec=/kubectl -- \
          --kubeconfig=/etc/kubernetes/kubeconfig \
          drain $(hostname) \
          --ignore-daemonsets \
          --delete-local-data \
          --force'

        [Install]
        WantedBy=multi-user.target

write_files:

  - path: /etc/kubernetes/kubeconfig
    permissions: "0644"
    owner: core
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: http://127.0.0.1:8080
      users:
      - name: kubelet
      contexts:
      - context:
          cluster: local
          user: kubelet

  - path: /etc/kubernetes/config/authn.yaml
    permissions: 0644
    owner: root:root
    content: |
      clusters:
        - name: authz-webhook
          cluster:
            server: http://127.0.0.1:8081/authentication
      users:
        - name: authz-webhook-client
          user:
            token: notused
      current-context: authz-webhook
      contexts:
      - context:
          cluster: authz-webhook
          user: authz-webhook-client
        name: authz-webhook

  - path: /etc/kubernetes/config/authz.yaml
    permissions: 0644
    owner: root:root
    content: |
      clusters:
        - name: authz-webhook
          cluster:
            server: http://127.0.0.1:8081/authorization
      users:
        - name: authz-webhook-client
      current-context: authz-webhook
      contexts:
      - context:
          cluster: authz-webhook
          user: authz-webhook-client
        name: authz-webhook

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-proxy
        namespace: kube-system
        labels:
          application: kube-proxy
          version: v1.7.4_coreos.0
        annotations:
          rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
      spec:
        hostNetwork: true
        containers:
        - name: kube-proxy
          image: registry.opensource.zalan.do/teapot/hyperkube:v1.7.4_coreos.0
          command:
          - /hyperkube
          - proxy
          - --master=http://127.0.0.1:8080
          - --feature-gates=AllAlpha=true
          - --v=2
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 50m
              memory: 75Mi
            limits:
              cpu: 200m
              memory: 200Mi
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /var/run/dbus
            name: dbus
            readOnly: false
        volumes:
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - hostPath:
            path: /var/run/dbus
          name: dbus

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        labels:
          application: kube-apiserver
          version: v1.7.4_coreos.0
        annotations:
          kubernetes-log-watcher/scalyr-parser: |
            [{"container": "webhook", "parser": "json-structured-log"}]
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: registry.opensource.zalan.do/teapot/hyperkube:v1.7.4_coreos.0
          command:
          - /hyperkube
          - apiserver
          - --apiserver-count={{ APISERVER_COUNT }}
          - --bind-address=0.0.0.0
          - --insecure-bind-address=0.0.0.0
          - --etcd-servers=http://127.0.0.1:2379
          - --etcd-prefix={{ APISERVER_ETCD_PREFIX }}
          - --storage-backend={{ APISERVER_STORAGE_BACKEND }}
          - --storage-media-type={{ APISERVER_STORAGE_MEDIA_TYPE }}
          - --allow-privileged=true
          - --service-cluster-ip-range=10.3.0.0/24
          - --secure-port=443
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,PodSecurityPolicy,ImagePolicyWebhook
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1=true,extensions/v1beta1/podsecuritypolicy=true,imagepolicy.k8s.io/v1alpha1=true,authorization.k8s.io/v1beta1=true
          - --authentication-token-webhook-config-file=/etc/kubernetes/config/authn.yaml
          - --cloud-provider=aws
          - --authorization-mode=Webhook
          - --authorization-webhook-config-file=/etc/kubernetes/config/authz.yaml
          - --admission-control-config-file=/etc/kubernetes/config/image-policy-webhook.yaml
          - --feature-gates=AllAlpha=true
          - --anonymous-auth=false
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 120
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes/config
            name: kubernetes-configs
            readOnly: true
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
            limits:
              cpu: 200m
              memory: 1Gi
        - image: registry.opensource.zalan.do/teapot/k8s-authnz-webhook:v0.2.3
          name: webhook
          ports:
          - containerPort: 8081
          livenessProbe:
            httpGet:
              path: /healthcheck
              port: 8081
            initialDelaySeconds: 30
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /healthcheck
              port: 8081
            initialDelaySeconds: 5
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          args:
            - --tokens-file=/etc/kubernetes/config/tokenfile.csv
          env:
            - name: USERS_API_URL
              value: https://users.auth.zalando.com
            - name: CLUSTER_ID
              value: {{WEBHOOK_ID}}
            - name: TOKEN_INTROSPECTION_URL
              value: http://127.0.0.1:9021/oauth2/introspect
            - name: USER_GROUPS
              value: stups_autobahn=Administrator,stups_deployment-controller-ui=ReadOnly,credentials-provider=Administrator,credprov-kube-ops-view-read-only-token=ReadOnly,credprov-cluster-lifecycle-manager-cluster-rw-token=Administrator,credprov-cluster-lifecycle-manager-test-cluster-rw-token=Administrator,credprov-cdp-controller-cluster-token=PowerUser,credprov-api-discovery-k8s-cluster-read-only-token=ReadOnly
            - name: BUSINESS_PARTNER_IDS
              value: {{ APISERVER_BUSINESS_PARTNER_IDS }}
          volumeMounts:
          - mountPath: /etc/kubernetes/config
            name: kubernetes-configs
            readOnly: true
        - image: registry.opensource.zalan.do/foundation/platform-iam-tokeninfo:4feeb5a
          name: tokeninfo
          ports:
            - containerPort: 9021
          livenessProbe:
            httpGet:
              path: /health
              port: 9021
            periodSeconds: 3
            failureThreshold: 2
          readinessProbe:
            httpGet:
              path: /health
              port: 9021
            periodSeconds: 3
            failureThreshold: 2
          resources:
            limits:
              cpu: 4000m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 20Mi
          env:
            - name: OPENID_PROVIDER_CONFIGURATION_URL
              value: https://identity.zalando.com/.well-known/openid-configuration
            - name: ENABLE_INTROSPECTION
              value: "true"
        - image: registry.opensource.zalan.do/teapot/image-policy-webhook:v0.2.2
          name: image-policy-webhook
          args:
          - --policy={{ IMAGE_POLICY }}
          ports:
          - containerPort: 8083
        - name: nginx
          image: registry.opensource.zalan.do/teapot/nginx:1.11.5
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 50m
              memory: 50Mi
          volumeMounts:
          - name: config-volume
            mountPath: /etc/nginx
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /etc/kubernetes/config
          name: kubernetes-configs
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host
        - hostPath:
            path: /etc/kubernetes/nginx
          name: config-volume

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        labels:
          application: kube-controller-manager
          version: v1.7.4_coreos.0
      spec:
        containers:
        - name: kube-controller-manager
          image: registry.opensource.zalan.do/teapot/hyperkube:v1.7.4_coreos.0
          command:
          - /hyperkube
          - controller-manager
          - --kubeconfig=/etc/kubernetes/controller-kubeconfig
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider=aws
          - --feature-gates=AllAlpha=true
          - --cloud-config=/etc/kubernetes/cloud-config.ini
          - --use-service-account-credentials=true
          resources:
            limits:
              cpu: 200m
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 100Mi
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
          - mountPath: /etc/kubernetes
            name: kubeconfig
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /etc/kubernetes
          name: kubeconfig
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
        labels:
          application: kube-scheduler
          version: v1.7.4_coreos.0
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: registry.opensource.zalan.do/teapot/hyperkube:v1.7.4_coreos.0
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --feature-gates=AllAlpha=true
          resources:
            limits:
              cpu: 250m
              memory: 250Mi
            requests:
              cpu: 100m
              memory: 100Mi
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15

  - path: /etc/kubernetes/manifests/rescheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: rescheduler-v0.3.1
        namespace: kube-system
        labels:
          application: rescheduler
          version: v0.3.1
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "Rescheduler"
      spec:
        hostNetwork: true
        containers:
        - image: registry.opensource.zalan.do/teapot/rescheduler:v0.3.1
          name: rescheduler
          resources:
            requests:
              cpu: 10m
              memory: 100Mi
          command:
          - /rescheduler
          args:
          - --running-in-cluster=false

  - path: /etc/kubernetes/nginx/nginx.conf
    content: |
      user nginx;
      worker_processes 1;
      error_log /dev/stdout info;
      events {
        worker_connections 1024;
      }
      http {
        default_type application/octet-stream;
        server_tokens off;
        access_log off;
        server {
          listen 8082;
          server_name _;
          location / {
            if ($request_method != GET) {
              return 403;
            }
            proxy_pass_request_body off;
            proxy_pass http://127.0.0.1:8080;
          }
          location ~ /secrets(/|$) {
            return 403 "unauthorized";
          }
        }
      }

  - path: /opt/bin/gen-controller-manager-config.sh
    owner: root:root
    permissions: 0755
    content: |
      #!/bin/bash
      token="$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')"
      cat << EOF > /etc/kubernetes/controller-kubeconfig
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          server: https://127.0.0.1
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      contexts:
      - name: local
        context:
          cluster: local
          user: kube-controller-manager
      current-context: local
      users:
      - name: kube-controller-manager
        user:
          token: ${token}
      EOF

      echo "${token},kube-controller-manager,kube-controller-manager" >> /etc/kubernetes/config/tokenfile.csv

  - path: /etc/kubernetes/config/tokenfile.csv
    content: |
      {{WORKER_SHARED_SECRET}},kubelet,kubelet

  - path: /etc/kubernetes/config/image-policy-webhook.yaml
    permissions: 0644
    owner: root:root
    content: |
      imagePolicy:
        kubeConfigFile: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
        allowTTL: 2
        denyTTL: 2
        retryBackoff: 500
        defaultAllow: false

  - path: /etc/kubernetes/config/image-policy-webhook-kubeconfig.yaml
    permissions: 0644
    owner: root:root
    content: |
      clusters:
        - name: image-policy-webhook
          cluster:
            server: http://127.0.0.1:8083/admit
      users:
        - name: image-policy-webhook
      current-context: image-policy-webhook
      contexts:
      - context:
          cluster: image-policy-webhook
          user: image-policy-webhook
        name: image-policy-webhook

  - path: /etc/kubernetes/ssl/ca.pem
    encoding: gzip+base64
    content: {{CA_CERT}}

  - path: /etc/kubernetes/ssl/apiserver.pem
    encoding: gzip+base64
    content: {{APISERVER_CERT}}

  - path: /etc/kubernetes/ssl/apiserver-key.pem
    encoding: gzip+base64
    content: {{APISERVER_KEY}}

  - path: /etc/kubernetes/ssl/worker.pem
    encoding: gzip+base64
    content: {{WORKER_CERT}}

  - path: /etc/kubernetes/ssl/worker-key.pem
    encoding: gzip+base64
    content: {{WORKER_KEY}}

  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }

  - path: /etc/kubernetes/cloud-config.ini
    content: |
      [global]
      DisableSecurityGroupIngress = true

  - path: /opt/bin/dockercfg.sh
    owner: root:root
    permissions: 0755
    content: |
      #!/bin/bash
      iid="instance-identity-document:$(curl http://169.254.169.254/latest/dynamic/instance-identity/pkcs7)"
      iid="$(echo -n "$iid" | base64 -w 0)"
      cat << EOF > /root/.docker/config.json
      {
        "auths": {
          "https://pierone.stups.zalan.do": {
            "auth": "${iid}"
          }
        }
      }
      EOF

  - path: /home/core/.toolboxrc
    owner: core
    content: |
      TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
      TOOLBOX_DOCKER_TAG=v0.0.4

  - path: /root/.toolboxrc
    owner: root
    content: |
      TOOLBOX_DOCKER_IMAGE=registry.opensource.zalan.do/teapot/microscope
      TOOLBOX_DOCKER_TAG=v0.0.4
